"""
FX AI EA è‡ªå‹•ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° v8 - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
  ãƒ»æœ€åˆã® 500 ä»¶: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ (æ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º)
  ãƒ»501 ä»¶ä»¥é™: 75% éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (TOP çµæœã‚’äº¤å‰ãƒ»çªç„¶å¤‰ç•°) + 25% ãƒ©ãƒ³ãƒ€ãƒ 
  ãƒ»VRAM / GPU ä½¿ç”¨ç‡ã‚’ç›£è¦–ã—ã¦å‹•çš„ã«ä¸¦åˆ—æ•°ã‚’æ±ºå®š
  ãƒ»åœæ­¢æ¡ä»¶ãªã— (stop.flag ãŒç½®ã‹ã‚Œã‚‹ã¾ã§ç„¡é™ç¶™ç¶š)
  ãƒ»TOP100 ãƒ¢ãƒ‡ãƒ«ä¿å­˜ + SR / DD / è³‡ç”£æ›²ç·šãƒ¬ãƒãƒ¼ãƒˆ
"""
import os, subprocess, sys, json, shutil, time, random, threading, signal, platform
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))
from feature_sets import FEATURE_SETS

PY        = sys.executable
TRAIN_PY  = Path(__file__).parent / 'train.py'
OUT_DIR   = Path(__file__).parent

_WORKSPACE    = Path('/workspace') if Path('/workspace').exists() else OUT_DIR.parent
STOP_FLAG     = _WORKSPACE / 'stop.flag'
TRIALS_DIR    = OUT_DIR / 'trials'
TOP_CACHE_DIR = OUT_DIR / 'top_cache'
TOP_DIR       = OUT_DIR / 'top100'
ALL_RESULTS   = OUT_DIR / 'all_results.json'
PROGRESS_JSON = OUT_DIR / 'progress.json'
BEST_ONNX     = OUT_DIR / 'fx_model_best.onnx'
BEST_NORM     = OUT_DIR / 'norm_params_best.json'
BEST_JSON     = OUT_DIR / 'best_result.json'

# â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ (åœæ­¢â†’å†é–‹ç”¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ­ãƒ¼ã‚«ãƒ«: /workspace/data/checkpoint/ ã«å®šæœŸä¿å­˜
# S3: ç’°å¢ƒå¤‰æ•° S3_* ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã° Sakura ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚‚ä¿å­˜
CHECKPOINT_DIR      = _WORKSPACE / 'data' / 'checkpoint'
CHECKPOINT_INTERVAL = 600   # ç§’ (10åˆ†ã”ã¨ã«ä¿å­˜)
CHECKPOINT_EVERY_N  = 10    # ä»¶ (10è©¦è¡Œå®Œäº†ã”ã¨ã«ä¿å­˜)

S3_ENDPOINT  = os.environ.get('S3_ENDPOINT',   '')   # ä¾‹: https://s3.isk01.sakurastorage.jp
S3_ACCESS_KEY= os.environ.get('S3_ACCESS_KEY',  '')
S3_SECRET_KEY= os.environ.get('S3_SECRET_KEY',  '')
S3_BUCKET    = os.environ.get('S3_BUCKET',      'fxea')
S3_PREFIX    = os.environ.get('S3_PREFIX',      'mix')   # ä¸¡ãƒãƒ¼ãƒ‰å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€
S3_ENABLED   = bool(S3_ENDPOINT and S3_ACCESS_KEY and S3_SECRET_KEY)

# â”€â”€ Google Drive å…±æœ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ (S3 ã‚ˆã‚Šå„ªå…ˆ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import gdrive as _gdrive
GDRIVE_ENABLED = _gdrive.GDRIVE_ENABLED


def remote_upload(local_path: Path, rel_key: str) -> bool:
    """GDrive > S3 ã®å„ªå…ˆé †ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    if GDRIVE_ENABLED:
        return _gdrive.upload(local_path, rel_key)
    if S3_ENABLED:
        return s3_upload(local_path, rel_key)
    return False


def remote_download(rel_key: str, local_path: Path) -> bool:
    """GDrive > S3 ã®å„ªå…ˆé †ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if GDRIVE_ENABLED:
        return _gdrive.download(rel_key, local_path)
    if S3_ENABLED:
        return s3_download(rel_key, local_path)
    return False


def remote_list_node_keys(glob_prefix: str) -> list[str]:
    """å…¨ãƒãƒ¼ãƒ‰ã®åŒç¨®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ (GDrive > S3)"""
    if GDRIVE_ENABLED:
        return _gdrive.list_node_keys(glob_prefix)
    if S3_ENABLED:
        return s3_list_node_keys(glob_prefix)
    return []


def remote_list_top100_keys() -> list[str]:
    """top100_* ä»¥ä¸‹ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ç›¸å¯¾ãƒ‘ã‚¹ä¸€è¦§ (GDrive > S3)"""
    if GDRIVE_ENABLED:
        return _gdrive.list_keys_recursive('top100_')
    if S3_ENABLED:
        raw = s3_list_keys('top100_')
        return [k[len(S3_PREFIX)+1:] for k in raw]
    return []


def remote_list_best_keys() -> list[str]:
    """best_* ä»¥ä¸‹ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ç›¸å¯¾ãƒ‘ã‚¹ä¸€è¦§ (GDrive > S3)"""
    if GDRIVE_ENABLED:
        return _gdrive.list_keys_recursive('best_')
    if S3_ENABLED:
        return s3_list_node_keys('best_')
    return []


def REMOTE_ENABLED() -> bool:
    return GDRIVE_ENABLED or S3_ENABLED

# â”€â”€ ãƒãƒ¼ãƒ‰ID (GTX / H100 / CPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 ä¸Šã§ãƒãƒ¼ãƒ‰ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é›¢ã™ã‚‹ã“ã¨ã§ç«¶åˆã‚’å›é¿ã™ã‚‹
def _detect_node_id() -> str:
    """ãƒ‡ãƒã‚¤ã‚¹åã‹ã‚‰ãƒãƒ¼ãƒ‰IDã‚’è‡ªå‹•æ±ºå®šã€‚ç’°å¢ƒå¤‰æ•° NODE_ID ã§ä¸Šæ›¸ãå¯èƒ½"""
    nid = os.environ.get('NODE_ID', '').strip()
    if nid:
        return nid.lower()
    # TPU æ¤œå‡º (torch_xla ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ)
    try:
        import torch_xla.core.xla_model as xm  # type: ignore
        dev_str = str(xm.xla_device()).lower()
        tpu_type = os.environ.get('TPU_NAME', os.environ.get('TPU_ACCELERATOR_TYPE', 'tpu'))
        # tpu_type ä¾‹: 'v4-8', 'v5litepod-8', 'v6e-1', 'trillium'
        for ver in ('v6e', 'v5p', 'v5e', 'v5litepod', 'v4', 'v3', 'trillium'):
            if ver in tpu_type.lower():
                return f'tpu_{ver}'
        return 'tpu'
    except Exception:
        pass
    # GPU æ¤œå‡º
    try:
        import torch
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0).lower()
            if 'h200' in name:     return 'h200'
            if 'h100' in name:     return 'h100'
            if 'a100' in name:     return 'a100'
            if '3090' in name:     return 'rtx3090'
            if '4090' in name:     return 'rtx4090'
            if '1080' in name:     return 'gtx1080ti'
            return name.replace(' ', '_')[:12]
    except Exception:
        pass
    return 'h100' if os.environ.get('H100_MODE', '0') == '1' else 'gtx1080ti'


def _auto_gpu_config(node_id: str) -> tuple[str, float, float, int]:
    """å®Ÿéš›ã®ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒ¢ãƒªã‚’èª­ã¿å–ã‚Šã€æœ€é©ãªä¸¦åˆ—æ•°ã¨å‰²å½“ã‚’è‡ªå‹•è¨ˆç®—ã™ã‚‹ã€‚

    ãƒ†ã‚£ã‚¢åŸºæº–:
      tpu    : Google TPU (v3/v4/v5/Trillium) â€” HBM â‰¥ 16 GB/chip
      xlarge : VRAM 120 GB+  (H200 SXM5  141 GB)
      large  : VRAM  60 GB+  (H100 80 GB / A100 80 GB / H200 NVL 94 GB)
      medium : VRAM  30 GB+  (A100 40 GB)
      small  : VRAM  14 GB+  (RTX 3090/4090  24 GB)
      micro  : VRAM   0 GB+  (GTX 1080 Ti 11 GB / ãã®ä»–)

    Returns: (tier, total_mem_gb, vram_per_trial_gb, max_parallel)
    """
    # TPU æ¤œå‡º
    if node_id.startswith('tpu'):
        try:
            import torch_xla.core.xla_model as xm  # type: ignore
            # TPU v4/v5 ã¯ 1ãƒãƒƒãƒ—ã‚ãŸã‚Š 32 GB HBM
            # Trillium (v6e) ã¯ 1ãƒãƒƒãƒ—ã‚ãŸã‚Š 32 GB
            num_devices = int(os.environ.get('TPU_NUM_DEVICES', '4'))
            mem_per_chip = {
                'tpu_v3': 16.0, 'tpu_v4': 32.0,
                'tpu_v5e': 16.0, 'tpu_v5p': 95.0,
                'tpu_trillium': 32.0,
                'tpu_v6e': 32.0,   # Trillium (v6e) = 32 GB HBM/chip
            }.get(node_id, 32.0)
            total_gb = mem_per_chip * num_devices
            # TPU ã¯ä¸¦åˆ—åº¦ã‚’ 1ãƒãƒƒãƒ— = 1è©¦è¡Œ ã¨ã—ã¦è¨ˆç®—
            vpt = mem_per_chip * 0.75
            par = max(1, num_devices)
            return 'tpu', total_gb, vpt, par
        except Exception:
            pass

    total_gb = 0.0
    try:
        import torch
        if torch.cuda.is_available():
            total_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    except Exception:
        pass

    if total_gb <= 0:
        _fallback: dict[str, float] = {
            'h200': 141.0, 'h100': 80.0, 'a100': 80.0,
            'rtx4090': 24.0, 'rtx3090': 24.0, 'gtx1080ti': 11.0,
        }
        total_gb = _fallback.get(node_id, 11.0)

    if   total_gb >= 120: tier = 'xlarge'
    elif total_gb >=  60: tier = 'large'
    elif total_gb >=  30: tier = 'medium'
    elif total_gb >=  14: tier = 'small'
    else:                 tier = 'micro'

    vpt_map = {'xlarge': 12.0, 'large': 10.0, 'medium': 8.0, 'small': 7.0, 'micro': 8.0}
    vpt = vpt_map[tier]
    par = max(1, int(total_gb * 0.85 / vpt))

    return tier, total_gb, vpt, par


NODE_ID = _detect_node_id()   # ã“ã®ãƒãƒ¼ãƒ‰ã®è­˜åˆ¥å­ (ä¾‹: 'h100', 'gtx1080ti')
GPU_NAME = os.environ.get("GPU_NAME", NODE_ID.upper())  # GPU display name for dashboard
_GPU_TIER, _GPU_VRAM_GB, _VPT_DEFAULT, _PAR_DEFAULT = _auto_gpu_config(NODE_ID)


def _s3_client():
    import boto3
    from botocore.config import Config
    return boto3.client(
        's3',
        endpoint_url      = S3_ENDPOINT,
        aws_access_key_id = S3_ACCESS_KEY,
        aws_secret_access_key = S3_SECRET_KEY,
        region_name       = os.environ.get('S3_REGION', 'jp-north-1'),
        config            = Config(connect_timeout=10, read_timeout=60,
                                   retries={'max_attempts': 2}),
    )


def s3_upload(local_path: Path, s3_key: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ S3 ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã‚’æŠ•ã’ãš False ã‚’è¿”ã™"""
    try:
        _s3_client().upload_file(str(local_path), S3_BUCKET,
                                 f'{S3_PREFIX}/{s3_key}')
        return True
    except Exception as e:
        print(f'  [S3] uploadå¤±æ•— {s3_key}: {e}')
        return False


def s3_download(s3_key: str, local_path: Path) -> bool:
    """S3 ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚å¤±æ•—ã—ãŸã‚‰ False ã‚’è¿”ã™"""
    try:
        local_path.parent.mkdir(parents=True, exist_ok=True)
        _s3_client().download_file(S3_BUCKET, f'{S3_PREFIX}/{s3_key}',
                                   str(local_path))
        return True
    except Exception as e:
        print(f'  [S3] downloadå¤±æ•— {s3_key}: {e}')
        return False


def s3_list_keys(prefix: str = '') -> list:
    """S3_PREFIX/prefix ä»¥ä¸‹ã®ã‚­ãƒ¼ä¸€è¦§ã‚’è¿”ã™"""
    try:
        full_prefix = f'{S3_PREFIX}/{prefix}' if prefix else S3_PREFIX + '/'
        paginator = _s3_client().get_paginator('list_objects_v2')
        keys = []
        for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=full_prefix):
            for obj in page.get('Contents', []):
                keys.append(obj['Key'])
        return keys
    except Exception as e:
        print(f'  [S3] listå¤±æ•—: {e}')
        return []


def s3_node_key(name: str) -> str:
    """ã“ã®ãƒãƒ¼ãƒ‰å°‚ç”¨ã® S3 ã‚­ãƒ¼ã‚’è¿”ã™ (ä¾‹: 'results_h100.json')"""
    return name.replace('NODE_ID', NODE_ID)


def s3_list_node_keys(glob_prefix: str) -> list[str]:
    """å…¨ãƒãƒ¼ãƒ‰ã®åŒç¨®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ (ä¾‹: glob_prefix='results_') â†’ ['results_h100.json', 'results_gtx1080ti.json']"""
    all_keys = s3_list_keys('')
    prefix_full = f'{S3_PREFIX}/{glob_prefix}'
    return [k[len(S3_PREFIX)+1:] for k in all_keys if k.startswith(prefix_full)]

TOP_N              = 100
RANDOM_PHASE_LIMIT = 30     # ã“ã®ä»¶æ•°ã¾ã§ã¯ç´”ãƒ©ãƒ³ãƒ€ãƒ ã€ä»¥é™ã¯ 10åˆ†äº¤äº’ãƒ¢ãƒ¼ãƒ‰
GA_PARENT_POOL     = 20     # è¦ªå€™è£œã‚’ä¸Šä½ä½•ä»¶ã‹ã‚‰é¸ã¶ã‹

# â”€â”€ 10åˆ†äº¤äº’ãƒ¢ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ 10åˆ† â†’ GA 10åˆ† â†’ ãƒ©ãƒ³ãƒ€ãƒ  10åˆ† â†’ ... ã‚’ç¹°ã‚Šè¿”ã™
MODE_SWITCH_SEC    = 600    # 10åˆ† = 600ç§’
_mode_start_time   = 0.0    # main() ã§ time.time() ã‚’è¨­å®š

# é‡è¦ç‰¹å¾´é‡ GA ãƒ•ã‚§ãƒ¼ãƒãƒ£: ä¸Šä½ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é‡è¦–ç‰¹å¾´é‡ã‚’åé›†ã—ã¦ GA ã«ä½¿ç”¨
IMP_FEAT_TOP_K     = 15     # å„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å–ã‚Šå‡ºã™é‡è¦ç‰¹å¾´é‡æ•°
IMP_FEAT_POOL_SIZE = 30     # é‡è¦ç‰¹å¾´é‡ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º (å¤šã‚ã«æŒã¤)
_important_features: list[str] = []   # é›†è¨ˆã—ãŸé‡è¦ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ (æ›´æ–°ã•ã‚Œã‚‹)
_important_scores: dict = {}          # ç‰¹å¾´é‡å â†’ é‡è¦åº¦ã‚¹ã‚³ã‚¢ (é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨)

# â”€â”€ 2ãƒ•ã‚§ãƒ¼ã‚ºGA ã®å‰²åˆ (GA_RATIO å†…ã®å†…è¨³) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GA_feat : feat_set ã®ã¿å¤‰ãˆã¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›ºå®šã§ç‰¹å¾´é‡ã‚’æ¢ç´¢
# GA_param: feat_set å›ºå®šã§ãƒã‚¤ãƒ‘ãƒ©ã®ã¿å¾®èª¿æ•´
# GA_cross: 2è¦ªã®äº¤å‰ (å¤šæ§˜æ€§ç¶­æŒ)
# åˆè¨ˆ = 1.0
GA_FEAT_RATIO  = 0.40   # ç‰¹å¾´é‡æ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º
GA_PARAM_RATIO = 0.40   # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º
GA_CROSS_RATIO = 0.20   # äº¤å‰ãƒ•ã‚§ãƒ¼ã‚º
# large/xlarge/medium/tpu = H100_MODE (å¤§ãƒ¢ãƒ‡ãƒ«ãƒ»é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æœ‰åŠ¹åŒ–)
# ç’°å¢ƒå¤‰æ•° H100_MODE=1 ã§å¼·åˆ¶æœ‰åŠ¹ã€H100_MODE=0 ã§å¼·åˆ¶ç„¡åŠ¹ã‚‚å¯èƒ½
_h100_env = os.environ.get('H100_MODE', '').strip()
H100_MODE = (
    (_h100_env == '1') or
    (_h100_env != '0' and _GPU_TIER in ('medium', 'large', 'xlarge', 'tpu'))
)
TPU_MODE = (_GPU_TIER == 'tpu')

# MAX_PARALLEL / VRAM_PER_TRIAL:
#   0 ã¾ãŸã¯æœªè¨­å®š â†’ GPU VRAM ã‹ã‚‰è‡ªå‹•è¨ˆç®—
#   1ä»¥ä¸Šã®æ•°å€¤   â†’ ãã®å€¤ã‚’å¼·åˆ¶ä½¿ç”¨
def _resolve_int_env(key: str, default: int) -> int:
    v = os.environ.get(key, '0').strip()
    return int(v) if v not in ('0', '', 'auto') else default

def _resolve_float_env(key: str, default: float) -> float:
    v = os.environ.get(key, '0').strip()
    return float(v) if v not in ('0', '', 'auto') else default

MAX_PARALLEL   = _resolve_int_env('MAX_PARALLEL',   _PAR_DEFAULT)
VRAM_PER_TRIAL = _resolve_float_env('VRAM_PER_TRIAL', _VPT_DEFAULT)

# â”€â”€ ãƒ•ãƒªãƒ¼ã‚ºæ¤œçŸ¥: GPUç„¡ä½¿ç”¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ãƒ•ã‚§ãƒ¼ã‚ºã« DATA_PREP_BUDGET ç§’ã®çŒ¶äºˆã‚’ä¸ãˆã€
# ãã‚Œä»¥é™ã‚‚ GPU ã‚’ä½¿ã£ã¦ã„ãªã‘ã‚Œã°å¼·åˆ¶çµ‚äº†
DATA_PREP_BUDGET  = 600    # ç§’: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®æœ€å¤§è¨±å®¹æ™‚é–“ (10åˆ†)
NO_GPU_TIMEOUT    = 900    # ç§’: GPUä½¿ç”¨ãªã—ã§ã“ã‚Œä»¥ä¸Šâ†’å¼·åˆ¶çµ‚äº† (15åˆ†)
LAUNCH_INTERVAL   = 1      # ç§’: è©¦è¡ŒæŠ•å…¥é–“éš” (CUDAåˆæœŸåŒ–ã®é‡è¤‡ã‚’é˜²ã)

ARCHS = [
    'mlp', 'gru_attn', 'bigru', 'lstm_attn',
    'cnn', 'tcn', 'cnn_gru', 'transformer', 'resnet', 'inception',
]

# â”€â”€ GPU ãƒ†ã‚£ã‚¢åˆ¥ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# micro  : GTX 1080 Ti  (11 GB)
# small  : RTX 3090/4090 (24 GB)
# medium : A100 40 GB
# large  : H100 80 GB / A100 80 GB / H200 NVL 94 GB
# xlarge : H200 SXM5 141 GB

_HIDDEN_MICRO = {
    'mlp':         [32, 64, 128, 256, 512],
    'gru_attn':    [64, 128, 256, 512],
    'bigru':       [64, 128, 256],
    'lstm_attn':   [64, 128, 256, 512],
    'cnn':         [64, 128, 256, 512],
    'tcn':         [64, 128, 256, 512],
    'cnn_gru':     [64, 128, 256],
    'transformer': [64, 128, 256],
    'resnet':      [64, 128, 256, 512],
    'inception':   [64, 128, 256],
}
_HIDDEN_SMALL = {   # RTX 3090/4090: ä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
    'mlp':         [128, 256, 512, 1024],
    'gru_attn':    [128, 256, 512, 1024],
    'bigru':       [128, 256, 512],
    'lstm_attn':   [128, 256, 512, 1024],
    'cnn':         [128, 256, 512, 1024],
    'tcn':         [128, 256, 512, 1024],
    'cnn_gru':     [128, 256, 512],
    'transformer': [128, 256, 512],
    'resnet':      [128, 256, 512, 1024],
    'inception':   [128, 256, 512],
}
_HIDDEN_LARGE = {   # medium/large/xlarge: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
    'mlp':         [512, 1024, 2048],
    'gru_attn':    [256, 512, 1024],
    'bigru':       [256, 512, 1024],
    'lstm_attn':   [256, 512, 1024],
    'cnn':         [256, 512, 1024],
    'tcn':         [256, 512, 1024],
    'cnn_gru':     [256, 512, 1024],
    'transformer': [256, 512, 1024],
    'resnet':      [256, 512, 1024, 2048],
    'inception':   [256, 512, 1024],
}

_TIER_HIDDEN_MAP = {
    'micro':  _HIDDEN_MICRO,
    'small':  _HIDDEN_SMALL,
    'medium': _HIDDEN_LARGE,
    'large':  _HIDDEN_LARGE,
    'xlarge': _HIDDEN_LARGE,
}
HIDDEN_MAP_LOCAL = _HIDDEN_MICRO   # å¾Œæ–¹äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹
HIDDEN_MAP_H100  = _HIDDEN_LARGE   # å¾Œæ–¹äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹
HIDDEN_MAP = _TIER_HIDDEN_MAP[_GPU_TIER]

# ãƒãƒƒãƒã‚µã‚¤ã‚º: å¤§VRAM ã»ã©å¤§ãã„ãƒãƒƒãƒã‚’æ¢ç´¢
_TIER_BATCH = {
    'micro':  [64, 128, 256, 512],
    'small':  [128, 256, 512, 1024],
    'medium': [256, 512, 1024, 2048],
    'large':  [256, 512, 1024, 2048],
    'xlarge': [512, 1024, 2048, 4096],
}
BATCH_CHOICES = _TIER_BATCH[_GPU_TIER]

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: å¤§VRAM ã»ã©é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ¢ç´¢
_TIER_SEQ = {
    'micro':  [5, 8, 10, 15, 20],
    'small':  [8, 10, 15, 20, 30],
    'medium': [10, 15, 20, 30, 40],
    'large':  [10, 15, 20, 30, 40, 50],
    'xlarge': [15, 20, 30, 40, 50, 60],
}
SEQ_CHOICES = _TIER_SEQ[_GPU_TIER]

EPOCH_COUNT = 800   # GPU å…±é€š

# è©¦è¡Œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: å¤§ãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚‹
_TIER_TIMEOUT = {
    'micro':   600,   # 10åˆ†
    'small':   900,   # 15åˆ†
    'medium': 1200,   # 20åˆ†
    'large':  1800,   # 30åˆ†
    'xlarge': 2400,   # 40åˆ†
}
TRIAL_TIMEOUT = _TIER_TIMEOUT[_GPU_TIER]


# â”€â”€ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sample_params(rng: random.Random) -> dict:
    arch    = rng.choice(ARCHS)
    hidden  = rng.choice(HIDDEN_MAP[arch])
    layers  = rng.choice([1, 2, 3] if arch not in ('mlp', 'gru_attn') else [1, 2])
    dropout = round(rng.uniform(0.3, 0.6), 1)
    lr      = rng.choice([1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 2e-3])
    # å¤§ãƒ¢ãƒ‡ãƒ«(hiddenâ‰¥1024)ã¯ CUDA OOM é˜²æ­¢ã§å°ãƒãƒƒãƒä¸Šé™ã‚’è¨­ã‘ã‚‹
    # ä¸Šé™ã¯ãƒ†ã‚£ã‚¢ã® BATCH_CHOICES æœ€å¤§å€¤ã®åŠåˆ†
    if hidden >= 1024 and len(BATCH_CHOICES) > 1:
        safe_batches = BATCH_CHOICES[:max(1, len(BATCH_CHOICES) - 1)]
        batch = rng.choice(safe_batches)
    else:
        batch = rng.choice(BATCH_CHOICES)
    tp      = round(rng.uniform(1.5, 3.5), 1)
    sl      = round(rng.uniform(0.5, 1.5), 1)
    fwd     = rng.choice([10, 15, 20, 25, 30])
    thr     = round(rng.uniform(0.33, 0.50), 2)
    seq_len = rng.choice(SEQ_CHOICES)
    sched   = rng.choice(['onecycle', 'cosine', 'cosine'])
    wd      = rng.choice([1e-3, 1e-2, 5e-2, 1e-1])
    tm      = rng.choice([0, 0, 0, 12, 18, 12])
    if rng.random() < 0.25:
        n_feat   = rng.randint(2, 70)
        feat_set = -1
    else:
        feat_set = rng.randint(0, len(FEATURE_SETS) - 1)
        n_feat   = len(FEATURE_SETS[feat_set])
    seed = rng.randint(0, 9999)
    return dict(
        arch=arch, hidden=hidden, layers=layers, dropout=dropout,
        lr=lr, batch=batch, tp=tp, sl=sl, forward=fwd,
        threshold=thr, seq_len=seq_len, scheduler=sched,
        wd=wd, train_months=tm, feat_set=feat_set, n_features=n_feat,
        seed=seed, timeframe='H1', epochs=EPOCH_COUNT,
        label_type='triple_barrier',
    )


# â”€â”€ éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _apply_one_mutation(p: dict, key: str, rng: random.Random) -> None:
    """key ã«å¯¾å¿œã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’1ã¤å¤‰ç•°ã•ã›ã‚‹ (in-place)"""
    if key == 'arch':
        p['arch']    = rng.choice(ARCHS)
        p['hidden']  = rng.choice(HIDDEN_MAP[p['arch']])
    elif key == 'hidden':
        p['hidden']  = rng.choice(HIDDEN_MAP[p['arch']])
    elif key == 'layers':
        p['layers']  = rng.choice([1, 2, 3] if p['arch'] not in ('mlp','gru_attn') else [1,2])
    elif key == 'dropout':
        p['dropout'] = round(rng.uniform(0.2, 0.7), 1)
    elif key == 'lr':
        p['lr']      = rng.choice([1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 2e-3])
    elif key == 'batch':
        if p['hidden'] >= 1024 and len(BATCH_CHOICES) > 1:
            p['batch'] = rng.choice(BATCH_CHOICES[:max(1, len(BATCH_CHOICES) - 1)])
        else:
            p['batch'] = rng.choice(BATCH_CHOICES)
    elif key == 'tp':
        p['tp']      = round(rng.uniform(1.2, 4.0), 1)
    elif key == 'sl':
        p['sl']      = round(rng.uniform(0.5, 2.0), 1)
    elif key == 'forward':
        p['forward'] = rng.choice([10, 15, 20, 25, 30, 40])
    elif key == 'threshold':
        p['threshold'] = round(rng.uniform(0.33, 0.55), 2)
    elif key == 'seq_len':
        p['seq_len'] = rng.choice(SEQ_CHOICES)
    elif key == 'scheduler':
        p['sched']   = rng.choice(['onecycle', 'cosine'])
    elif key == 'wd':
        p['wd']      = rng.choice([1e-4, 1e-3, 1e-2, 5e-2, 1e-1])
    elif key == 'train_months':
        p['train_months'] = rng.choice([0, 0, 12, 18, 24, 12])
    elif key == 'feat_set':
        # ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚»ãƒƒãƒˆã‚’å¤‰ãˆã‚‹ (æ¢ç´¢å¤šæ§˜æ€§å‘ä¸Š)
        p['feat_set'] = rng.randint(0, 99)


def _mutate(params: dict, rng: random.Random) -> dict:
    """è¤‡æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ç•°ã•ã›ã‚‹ (1ã€œ3å€‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ)"""
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚³ãƒ”ãƒ¼ (trial/pf ç­‰ã®çµæœãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–)
    _hp_keys = ('arch', 'hidden', 'layers', 'dropout', 'lr', 'batch',
                'tp', 'sl', 'forward', 'threshold', 'seq_len',
                'scheduler', 'sched', 'wd', 'train_months', 'feat_set', 'n_features',
                'seed', 'epochs', 'timeframe', 'label_type')
    p = {k: v for k, v in params.items() if k in _hp_keys}
    mut_keys = [
        'arch', 'hidden', 'layers', 'dropout', 'lr', 'batch',
        'tp', 'sl', 'forward', 'threshold', 'seq_len',
        'scheduler', 'wd', 'train_months', 'feat_set',
    ]
    # å¤‰ç•°æ•°: å¤šæ§˜æ€§ã®ãŸã‚1ã€œ3å€‹
    n_mut = rng.choices([1, 2, 3], weights=[0.5, 0.35, 0.15])[0]
    chosen = rng.sample(mut_keys, n_mut)
    for key in chosen:
        _apply_one_mutation(p, key, rng)
    # arch/hidden ã®æ•´åˆæ€§ã‚’ä¿è¨¼
    if p['hidden'] not in HIDDEN_MAP.get(p['arch'], [p['hidden']]):
        p['hidden'] = rng.choice(HIDDEN_MAP[p['arch']])
    p['seed'] = rng.randint(0, 9999)
    return p


def _crossover(p1: dict, p2: dict, rng: random.Random) -> dict:
    """2 ã¤ã®è¦ªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ 1 ç‚¹äº¤å‰ã§æ··åˆ"""
    keys = [
        'arch', 'hidden', 'layers', 'dropout', 'lr', 'batch',
        'tp', 'sl', 'forward', 'threshold', 'seq_len',
        'scheduler', 'wd', 'train_months', 'feat_set', 'n_features',
    ]
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚³ãƒ”ãƒ¼ (trial/pf/strategy ç­‰ã®çµæœãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–)
    child = {k: p1[k] for k in keys if k in p1}
    for k in keys:
        if rng.random() < 0.5 and k in p2:
            child[k] = p2[k]
    # arch ã¨ hidden ã®çµ„ã¿åˆã‚ã›ãŒå´©ã‚Œã¦ã„ãŸã‚‰ä¿®æ­£
    if child['hidden'] not in HIDDEN_MAP.get(child['arch'], [child['hidden']]):
        child['hidden'] = rng.choice(HIDDEN_MAP[child['arch']])
    child['seed'] = rng.randint(0, 9999)
    child['epochs'] = EPOCH_COUNT
    child['timeframe'] = 'H1'
    child['label_type'] = 'triple_barrier'
    return child


def _tournament_select(pool: list, rng: random.Random, k: int = 4) -> dict:
    """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé¸æŠ: pool ã‹ã‚‰ k ä»¶ã‚’å¼•ã„ã¦ PF æœ€å¤§ã‚’è¿”ã™"""
    candidates = rng.sample(pool, min(k, len(pool)))
    return max(candidates, key=lambda r: r['pf'])


def _build_parent_pool(results: list) -> list:
    """æœ‰åŠ¹ãªçµæœã‹ã‚‰è¦ªãƒ—ãƒ¼ãƒ«ã‚’æ§‹ç¯‰ (archÃ—feat_set ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿)"""
    valid = [r for r in results if r.get('pf', 0) > 0 and r.get('trades', 0) >= 200]
    sorted_valid = sorted(valid, key=lambda x: -x['pf'])
    pool: list = []
    seen_arch_feat: set = set()
    for r in sorted_valid:
        key = (r.get('arch', '?'), r.get('feat_set', -1))
        if key not in seen_arch_feat or len(pool) < GA_PARENT_POOL // 2:
            pool.append(r)
            seen_arch_feat.add(key)
        if len(pool) >= GA_PARENT_POOL:
            break
    return pool


_GA_HP_KEYS = ('arch', 'hidden', 'layers', 'dropout', 'lr', 'batch',
               'tp', 'sl', 'forward', 'threshold', 'seq_len',
               'scheduler', 'sched', 'wd', 'train_months', 'feat_set', 'n_features',
               'seed', 'epochs', 'timeframe', 'label_type')


def ga_feat_explore(results: list, rng: random.Random) -> dict:
    """ãƒ•ã‚§ãƒ¼ã‚º1 â”€ ç‰¹å¾´é‡æ¢ç´¢
    è¦ªã® arch/hidden/lr/dropout ç­‰ã‚’å›ºå®šã— feat_set ã ã‘ã‚’å¤‰ãˆã‚‹ã€‚
    é‡è¦ç‰¹å¾´é‡ãƒ—ãƒ¼ãƒ«ãŒã‚ã‚Œã°ã€é‡è¤‡åº¦ã®é«˜ã„ feat_set ã‚’å„ªå…ˆã—ã¦é¸ã¶ã€‚
    """
    pool = _build_parent_pool(results)
    if not pool:
        return sample_params(rng)

    parent = _tournament_select(pool, rng)
    child  = {k: parent[k] for k in _GA_HP_KEYS if k in parent}
    orig_feat = parent.get('feat_set', -1)

    # é‡è¦ç‰¹å¾´é‡ã¨é‡è¤‡åº¦ãŒé«˜ã„ feat_set ã‚’å„ªå…ˆ
    if _important_features:
        from features import FEATURE_COLS
        imp_set = set(_important_features)
        # å„ feat_set ã®é‡è¦ç‰¹å¾´é‡é‡è¤‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scores = []
        for fi, fset in enumerate(FEATURE_SETS):
            if fi == orig_feat:
                scores.append(0.0)   # è¦ªã¨åŒã˜ã¯é™¤å¤–
                continue
            feat_names = set(FEATURE_COLS[j] for j in fset if j < len(FEATURE_COLS))
            overlap = len(feat_names & imp_set)
            scores.append(float(overlap) + 0.1)   # 0.1 ã¯ã‚¼ãƒ­é‡ã¿ã‚’é˜²ã
        total = sum(scores)
        if total > 0:
            weights = [s / total for s in scores]
            new_feat = rng.choices(range(len(FEATURE_SETS)), weights=weights)[0]
        else:
            new_feat = rng.randint(0, len(FEATURE_SETS) - 1)
    else:
        # é‡è¦ç‰¹å¾´é‡æœªé›†è¨ˆã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ 
        new_feat = orig_feat
        for _ in range(3):
            new_feat = rng.randint(0, len(FEATURE_SETS) - 1)
            if new_feat != orig_feat:
                break

    child['feat_set']   = new_feat
    child['n_features'] = len(FEATURE_SETS[new_feat])
    child['seed']       = rng.randint(0, 9999)
    return child


def ga_param_tune(results: list, rng: random.Random) -> dict:
    """ãƒ•ã‚§ãƒ¼ã‚º2 â”€ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    è¦ªã® feat_set ã‚’å›ºå®šã—ã€lr/dropout/tp/sl/threshold ç­‰ã®ãƒã‚¤ãƒ‘ãƒ©ã®ã¿å¤‰ãˆã‚‹ã€‚
    è‰¯ã„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä¿æŒã—ãŸã¾ã¾ç´°ã‹ã„æœ€é©åŒ–ã‚’è¡Œã†ã€‚
    """
    pool = _build_parent_pool(results)
    if not pool:
        return sample_params(rng)

    parent = _tournament_select(pool, rng)
    child  = {k: parent[k] for k in _GA_HP_KEYS if k in parent}

    # feat_setãƒ»arch ã¯å›ºå®šã€ãƒã‚¤ãƒ‘ãƒ©ã®ã¿ 1ã€œ2 å€‹å¤‰æ›´
    tune_keys = ['lr', 'dropout', 'tp', 'sl', 'threshold',
                 'batch', 'wd', 'forward', 'seq_len', 'layers',
                 'train_months', 'scheduler']
    n_mut  = rng.choices([1, 2, 3], weights=[0.45, 0.40, 0.15])[0]
    chosen = rng.sample(tune_keys, min(n_mut, len(tune_keys)))
    for key in chosen:
        _apply_one_mutation(child, key, rng)

    # arch ã¨ hidden ã®çµ„ã¿åˆã‚ã›æ•´åˆæ€§ã‚’ä¿è¨¼
    if child.get('hidden') not in HIDDEN_MAP.get(child.get('arch', ''), [child.get('hidden')]):
        child['hidden'] = rng.choice(HIDDEN_MAP[child['arch']])
    child['seed'] = rng.randint(0, 9999)
    return child


def ga_sample(results: list, rng: random.Random) -> tuple[dict, str]:
    """2ãƒ•ã‚§ãƒ¼ã‚ºGA: ç‰¹å¾´é‡æ¢ç´¢ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ äº¤å‰ ã®3ã‚µãƒ–æˆ¦ç•¥ã‚’è¿”ã™"""
    valid = [r for r in results if r.get('pf', 0) > 0 and r.get('trades', 0) >= 200]
    if len(valid) < 2:
        return sample_params(rng), 'random'

    r_val = rng.random()
    if r_val < GA_FEAT_RATIO:
        # ãƒ•ã‚§ãƒ¼ã‚º1: ç‰¹å¾´é‡æ¢ç´¢ (feat_set ã ã‘å¤‰ãˆã‚‹)
        return ga_feat_explore(results, rng), 'GA_feat'
    elif r_val < GA_FEAT_RATIO + GA_PARAM_RATIO:
        # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (feat_set å›ºå®šã§ãƒã‚¤ãƒ‘ãƒ©å¤‰æ›´)
        return ga_param_tune(results, rng), 'GA_param'
    else:
        # äº¤å‰: 2 è¦ªã‹ã‚‰å¤šæ§˜æ€§ã‚’ç”Ÿæˆ
        pool = _build_parent_pool(results)
        if len(pool) < 2:
            return sample_params(rng), 'random'
        p1 = _tournament_select(pool, rng)
        p2 = _tournament_select(pool, rng)
        return _crossover(p1, p2, rng), 'GA_cross'


def _current_mode() -> str:
    """10åˆ†ã”ã¨ã« 'random' / 'ga' ã‚’äº¤äº’ã«è¿”ã™"""
    if _mode_start_time <= 0:
        return 'random'
    elapsed = time.time() - _mode_start_time
    cycle   = int(elapsed // MODE_SWITCH_SEC)
    return 'ga' if cycle % 2 == 1 else 'random'


def _update_important_features(results: list) -> None:
    """ä¸Šä½ãƒ¢ãƒ‡ãƒ«ã® feature_importance ã‹ã‚‰é‡è¦ç‰¹å¾´é‡ãƒ—ãƒ¼ãƒ«ã‚’æ›´æ–°ã€‚
    PF é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆã—ã€å¤šæ§˜æ€§ã®ã‚ã‚‹ãƒ—ãƒ¼ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    """
    global _important_features, _important_scores
    from collections import defaultdict
    # PF > 0.9 ã®ãƒ¢ãƒ‡ãƒ«ã‚’æœ€å¤§30ä»¶å¯¾è±¡ï¼ˆPFé–¾å€¤ã¯ç·©ã‚ï¼‰
    valid = [r for r in results
             if r.get('pf', 0) > 0.9 and r.get('trades', 0) >= 200
             and r.get('feature_importance')]
    if not valid:
        # PF > 0 ã§ã‚‚è©¦ã¿ã‚‹ (åºç›¤ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        valid = [r for r in results
                 if r.get('pf', 0) > 0 and r.get('trades', 0) >= 200
                 and r.get('feature_importance')]
    top30 = sorted(valid, key=lambda x: -x['pf'])[:30]
    if not top30:
        return

    # PF ã§é‡ã¿ä»˜ã‘ã—ã¦ã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ
    scores: dict = defaultdict(float)
    for rank_i, r in enumerate(top30):
        pf_weight = r['pf'] ** 2   # PF ãŒé«˜ã„ã»ã©é‡ã¿ã‚’å¤§ãã
        for fname, score in (r.get('feature_importance') or [])[:IMP_FEAT_TOP_K]:
            if isinstance(fname, str):
                scores[fname] += score * pf_weight

    if not scores:
        return

    # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_feats = sorted(scores.items(), key=lambda x: -x[1])
    _important_features = [f for f, _ in sorted_feats[:IMP_FEAT_POOL_SIZE]]
    _important_scores   = {f: s for f, s in sorted_feats[:IMP_FEAT_POOL_SIZE]}


def _ga_sample_with_important_features(results: list, rng: random.Random) -> tuple[dict, str]:
    """é‡è¦ç‰¹å¾´é‡ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã— GA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨çµ„ã¿åˆã‚ã›ã‚‹ã€‚

    3ã¤ã®ãƒ¢ãƒ¼ãƒ‰:
      A (60%) imp_core   : é‡è¦åº¦ä¸Šä½ã‚’å¿…ãšå«ã¿ã€æ®‹ã‚Šã¯é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      B (25%) imp_wide   : é‡è¦ç‰¹å¾´é‡ + å¤šã‚ã®ãƒ©ãƒ³ãƒ€ãƒ è¿½åŠ  (å¤šæ§˜æ€§é‡è¦–)
      C (15%) imp_exploit: æœ€é‡è¦ç‰¹å¾´é‡ã®ã¿çµã‚Šè¾¼ã¿ (ç‰¹åŒ–å‹)
    å„ãƒ¢ãƒ¼ãƒ‰ã«å¯¾ã—ã¦ GA_param ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚é©ç”¨ã™ã‚‹ã€‚
    """
    global _important_features, _important_scores
    from features import FEATURE_COLS, N_FEATURES

    if len(_important_features) < 5:
        return ga_sample(results, rng)

    # é‡è¦ç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ—
    imp_idx  = [FEATURE_COLS.index(f) for f in _important_features if f in FEATURE_COLS]
    all_idx  = list(range(N_FEATURES))
    non_imp  = [i for i in all_idx if i not in imp_idx]

    mode_r = rng.random()
    if mode_r < 0.60:
        # ãƒ¢ãƒ¼ãƒ‰ A: ã‚³ã‚¢é‡è¦ç‰¹å¾´é‡ (ä¸Šä½ 10ã€œ15) + é‡ã¿ä»˜ãè¿½åŠ 
        core_lo  = min(10, len(imp_idx))
        core_hi  = min(15, len(imp_idx))
        core_n   = rng.randint(core_lo, max(core_lo, core_hi))
        core_idx = imp_idx[:core_n]
        if non_imp:
            extra_lo = min(5, len(non_imp))
            extra_hi = min(20, len(non_imp))
            extra_n  = rng.randint(extra_lo, max(extra_lo, extra_hi))
            extra    = rng.sample(non_imp, extra_n)
        else:
            extra = []
        feat_idx = sorted(set(core_idx + extra))
        mode_tag = 'imp_core'
    elif mode_r < 0.85:
        # ãƒ¢ãƒ¼ãƒ‰ B: é‡è¦ç‰¹å¾´é‡ã‹ã‚‰é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + å¤šã‚ãƒ©ãƒ³ãƒ€ãƒ 
        weights   = [_important_scores.get(FEATURE_COLS[i], 0.001) for i in imp_idx]
        total_w   = sum(weights) or 1.0
        weights   = [w / total_w for w in weights]
        k_lo      = max(1, min(5, len(imp_idx) // 2))
        k_imp     = rng.randint(k_lo, max(k_lo, len(imp_idx)))
        # é‡ã¿ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        chosen_imp = []
        pool_copy  = list(zip(imp_idx, weights))
        for _ in range(k_imp):
            if not pool_copy:
                break
            ws = [w for _, w in pool_copy]
            pick = rng.choices(range(len(pool_copy)), weights=ws)[0]
            chosen_imp.append(pool_copy[pick][0])
            pool_copy.pop(pick)
        if non_imp:
            extra_lo = min(5, len(non_imp))
            extra_hi = min(30, len(non_imp))
            extra_n  = rng.randint(extra_lo, max(extra_lo, extra_hi))
            extra    = rng.sample(non_imp, extra_n)
        else:
            extra = []
        feat_idx = sorted(set(chosen_imp + extra))
        mode_tag = 'imp_wide'
    else:
        # ãƒ¢ãƒ¼ãƒ‰ C: æœ€é‡è¦ç‰¹å¾´é‡ã®ã¿ (çµã‚Šè¾¼ã¿ãƒ»ç‰¹åŒ–)
        top_lo   = min(5, len(imp_idx))
        top_n    = rng.randint(top_lo, max(top_lo, min(12, len(imp_idx))))
        feat_idx = sorted(imp_idx[:top_n])
        mode_tag = 'imp_exploit'

    # GA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚åˆã‚ã›ã¦å–å¾—
    p, strategy = ga_sample(results, rng)
    p['feat_indices'] = feat_idx
    p.pop('feat_set',   None)   # feat_set ã¯ feat_indices ã§ä¸Šæ›¸ã
    p.pop('n_features', None)
    return p, f'{strategy}_{mode_tag}'


def next_params(results: list, rng: random.Random) -> tuple[dict, str]:
    """10åˆ†ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ â†”GAã‚’åˆ‡ã‚Šæ›¿ãˆã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æˆ¦ç•¥åã‚’è¿”ã™"""
    n = len(results)
    if n < RANDOM_PHASE_LIMIT:
        return sample_params(rng), 'random'
    mode = _current_mode()
    if mode == 'ga':
        return _ga_sample_with_important_features(results, rng)
    return sample_params(rng), 'random'


# â”€â”€ GPU æƒ…å ±å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gpu_info() -> dict:
    try:
        # nvidia-ml-py (pynvml ã®å¾Œç¶™ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸)
        from pynvml import (nvmlInit, nvmlDeviceGetHandleByIndex,
                            nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates)
        nvmlInit()
        h = nvmlDeviceGetHandleByIndex(0)
        m = nvmlDeviceGetMemoryInfo(h)
        u = nvmlDeviceGetUtilizationRates(h)
        return {
            'free_gb':  m.free  / 1e9,
            'total_gb': m.total / 1e9,
            'used_gb':  m.used  / 1e9,
            'gpu_pct':  u.gpu,
            'mem_pct':  round(m.used / m.total * 100),
        }
    except Exception:
        pass
    # pynvml å¤±æ•—æ™‚ â†’ torch ã‹ã‚‰ VRAM ã‚’å–å¾—
    try:
        import torch
        if torch.cuda.is_available():
            prop  = torch.cuda.get_device_properties(0)
            total = prop.total_memory / 1e9
            used  = (torch.cuda.memory_allocated(0) + torch.cuda.memory_reserved(0)) / 1e9
            return {'free_gb': total - used, 'total_gb': total, 'used_gb': used,
                    'gpu_pct': 0, 'mem_pct': round(used / max(total, 1) * 100)}
    except Exception:
        pass
    fallback_total = _GPU_VRAM_GB if _GPU_VRAM_GB > 0 else 11.0
    return {'free_gb': fallback_total, 'total_gb': fallback_total, 'used_gb': 0,
            'gpu_pct': 0, 'mem_pct': 0}


def get_gpu_compute_pids() -> set:
    """nvidia-smi ã§ç¾åœ¨ GPU è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ PID ã‚»ãƒƒãƒˆã‚’è¿”ã™"""
    try:
        r = subprocess.run(
            ['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader'],
            capture_output=True, text=True, timeout=10
        )
        pids = set()
        for line in r.stdout.strip().split('\n'):
            line = line.strip()
            if line.isdigit():
                pids.add(int(line))
        return pids
    except Exception:
        return set()


def get_max_parallel(n_running: int) -> int:
    """VRAM/GPU ä½¿ç”¨ç‡ã‹ã‚‰å‹•çš„ã«æœ€å¤§ä¸¦åˆ—æ•°ã‚’è¿”ã™"""
    if not H100_MODE:
        return MAX_PARALLEL
    gi = _gpu_info()
    total_gb = max(gi['total_gb'], 1)
    used_gb  = gi['used_gb']
    mem_pct  = used_gb / total_gb * 100

    # VRAMä½¿ç”¨ç‡ãŒ90%è¶…ãªã‚‰æ–°è¦èµ·å‹•ã‚’æŠ‘åˆ¶ (OOMé˜²æ­¢)
    if mem_pct > 90 and n_running > 0:
        return n_running  # ç¾çŠ¶ç¶­æŒã€è¿½åŠ èµ·å‹•ã—ãªã„
    # VRAMä½¿ç”¨ç‡ãŒ85%è¶…ãªã‚‰ä¿å®ˆçš„ã«ä¸¦åˆ—æ•°åˆ¶é™
    if mem_pct > 85:
        return max(1, min(n_running + 1, MAX_PARALLEL))
    # VRAM ç©ºãã‹ã‚‰æ ã‚’è¨ˆç®—
    vram_slots = max(1, int(gi['free_gb'] / VRAM_PER_TRIAL))
    # GPU ãŒé«˜è² è·ãªã‚‰ç¶­æŒ
    if gi['gpu_pct'] > 92 and n_running > 0:
        return n_running
    # VRAMä¸è¶³ã§ã‚‚æœ€ä½1ä¸¦åˆ—ã¯ä¿è¨¼ (ãƒ•ãƒªãƒ¼ã‚ºé˜²æ­¢)
    return max(1, min(MAX_PARALLEL, vram_slots))


# â”€â”€ TOP_N ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_trial_model(trial_no: int) -> None:
    """ç¾åœ¨ã® ONNX ã¨ norm_params ã‚’ top_cache ã«ä¿å­˜ (ãƒãƒ¼ãƒ‰IDãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ã)"""
    trial_dir = TRIALS_DIR / f'trial_{trial_no:06d}'
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: trial_{node_id}_{trial_no} ã§å…¨ãƒãƒ¼ãƒ‰é–“ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯
    cache_key = f'trial_{NODE_ID}_{trial_no:06d}'
    dest = TOP_CACHE_DIR / cache_key
    dest.mkdir(parents=True, exist_ok=True)
    for fname in ['fx_model.onnx', 'norm_params.json', 'report.html']:
        src = trial_dir / fname
        if src.exists():
            shutil.copy2(src, dest / fname)


def rebuild_top_n(results: list) -> None:
    """å…¨ãƒãƒ¼ãƒ‰ã® results ã‹ã‚‰ TOP_N ã‚’è¨ˆç®—ã—ã¦ top100/rank_XXX/ ã‚’å†æ§‹ç¯‰"""
    valid = [r for r in results
             if r.get('pf', 0) > 0 and r.get('trades', 0) >= 200]
    top_n = sorted(valid, key=lambda x: -x['pf'])[:TOP_N]
    TOP_DIR.mkdir(parents=True, exist_ok=True)
    for rank, r in enumerate(top_n, 1):
        tno  = r.get('trial', 0)
        nid  = r.get('node_id', NODE_ID)
        # ãƒãƒ¼ãƒ‰IDã¤ãã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã§æ¤œç´¢ (æ—§å½¢å¼ã«ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        src = TOP_CACHE_DIR / f'trial_{nid}_{tno:06d}'
        if not src.exists():
            src = TOP_CACHE_DIR / f'trial_{tno:06d}'   # æ—§å½¢å¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        dst = TOP_DIR / f'rank_{rank:03d}'
        if src.exists():
            if dst.exists():
                shutil.rmtree(dst)
            shutil.copytree(src, dst)
            (dst / 'result.json').write_text(
                json.dumps(r, indent=2, ensure_ascii=False), encoding='utf-8')


# â”€â”€ é›†ç´„ progress.json â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_progress(running: dict, results: list, best_pf: float, start: float) -> None:
    running_info = []
    gi = _gpu_info()
    for tno, info in list(running.items()):
        tp_file = info['trial_dir'] / 'trial_progress.json'
        tp = {}
        if tp_file.exists():
            try:
                tp = json.loads(tp_file.read_text(encoding='utf-8'))
            except Exception:
                pass
        running_info.append({
            'trial':       tno,
            'arch':        info['params'].get('arch', '?'),
            'hidden':      info['params'].get('hidden', '?'),
            'epoch':       tp.get('epoch', 0),
            'total_epochs':tp.get('total_epochs', EPOCH_COUNT),
            'train_loss':  tp.get('train_loss', 0.0),
            'val_loss':    tp.get('val_loss', 0.0),
            'accuracy':    tp.get('accuracy', 0.0),
            'phase':       tp.get('phase', 'running'),
            'strategy':    info.get('strategy', 'random'),
            'elapsed_sec': round(time.time() - info['start_time'], 0),
        })

    # æœ€è¿‘ã® trial çµæœ (epoch_log ç”¨ã«æœ€æ–° running trial ã® log ã‚’ä½¿ã†)
    epoch_log = []
    if running_info:
        latest = max(running_info, key=lambda x: x['trial'])
        tp_file = (TRIALS_DIR / f"trial_{latest['trial']:06d}" / 'trial_progress.json')
        if tp_file.exists():
            try:
                epoch_log = json.loads(tp_file.read_text(encoding='utf-8')).get('epoch_log', [])
            except Exception:
                pass

    n_done    = len(results)
    if n_done < RANDOM_PHASE_LIMIT:
        search_phase = 'random (åˆæœŸãƒ•ã‚§ãƒ¼ã‚º)'
    else:
        mode    = _current_mode()
        elapsed = time.time() - _mode_start_time if _mode_start_time > 0 else 0
        cycle   = int(elapsed // MODE_SWITCH_SEC)
        remain  = int(MODE_SWITCH_SEC - (elapsed % MODE_SWITCH_SEC))
        imp_tag = f' ğŸ¯é‡è¦ç‰¹å¾´é‡{len(_important_features)}å€‹æ´»ç”¨' if _important_features else ''
        if mode == 'ga':
            search_phase = f'ğŸ” GA ãƒ¢ãƒ¼ãƒ‰ (æ®‹{remain}ç§’â†’ãƒ©ãƒ³ãƒ€ãƒ åˆ‡æ›¿){imp_tag}'
        else:
            search_phase = f'ğŸ² ãƒ©ãƒ³ãƒ€ãƒ  ãƒ¢ãƒ¼ãƒ‰ (æ®‹{remain}ç§’â†’GAåˆ‡æ›¿){imp_tag}'
    # å…¨ãƒãƒ¼ãƒ‰ã®çµæœé›†è¨ˆ
    nodes_summary = {}
    for r in results:
        nid = r.get('node_id', NODE_ID)
        if nid not in nodes_summary:
            nodes_summary[nid] = {'count': 0, 'best_pf': 0.0}
        nodes_summary[nid]['count'] += 1
        if r.get('pf', 0) > nodes_summary[nid]['best_pf'] and r.get('trades', 0) >= 200:
            nodes_summary[nid]['best_pf'] = r.get('pf', 0)

    progress = {
        'phase':           'training' if running else 'waiting',
        'search_phase':    search_phase,
        'completed_count': n_done,
        'random_phase_limit': RANDOM_PHASE_LIMIT,
        'running_count':   len(running),
        'running_trials':  running_info,
        'best_pf':         best_pf,
        'target_pf':       0,
        'epoch_log':       epoch_log,
        'trial_results':   results[-200:],
        'start_time':      start,
        'elapsed_sec':     time.time() - start,
        'gpu_pct':         gi['gpu_pct'],
        'vram_used_gb':    round(gi['used_gb'], 1),
        'vram_total_gb':   round(gi['total_gb'], 1),
        'gpu_name':        GPU_NAME,
        'node_id':         NODE_ID,
        'nodes_summary':   nodes_summary,
        'important_features': _important_features[:10],   # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
        'message': (f"[{NODE_ID.upper()}] å®Ÿè¡Œä¸­: {len(running)}ä¸¦åˆ—  å®Œäº†: {n_done}ä»¶  "
                    f"ãƒ™ã‚¹ãƒˆ PF: {best_pf:.4f}  [{search_phase}]  "
                    f"GPU: {gi['gpu_pct']}%  VRAM: {gi['used_gb']:.1f}/{gi['total_gb']:.0f}GB"),
    }
    try:
        tmp = PROGRESS_JSON.with_suffix('.tmp')
        tmp.write_text(json.dumps(progress, ensure_ascii=False, indent=2), encoding='utf-8')
        tmp.replace(PROGRESS_JSON)
    except Exception as e:
        print(f"  [WARN] progress.json æ›¸ãè¾¼ã¿å¤±æ•—: {e}")


# â”€â”€ ä¸¦åˆ—ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ParallelTrainer:
    def __init__(self):
        self.running: dict = {}   # trial_no -> {proc, params, start_time, trial_dir, log_fh}
        self.lock = threading.Lock()

    def launch(self, trial_no: int, params: dict, best_pf: float, start_time: float,
               strategy: str = 'random'):
        trial_dir = TRIALS_DIR / f'trial_{trial_no:06d}'
        trial_dir.mkdir(parents=True, exist_ok=True)

        cmd = [PY, str(TRAIN_PY),
               '--trial',        str(trial_no),
               '--total_trials', '99999',
               '--best_pf',      str(best_pf),
               '--start_time',   str(start_time),
               '--out_dir',      str(trial_dir),
               ]
        for k, v in params.items():
            cmd += [f'--{k}', str(v)]

        log_fh = open(trial_dir / 'train.log', 'w', encoding='utf-8', buffering=1)
        proc   = subprocess.Popen(cmd, stdout=log_fh, stderr=subprocess.STDOUT)

        with self.lock:
            self.running[trial_no] = {
                'proc':       proc,
                'params':     params,
                'start_time': time.time(),
                'trial_dir':  trial_dir,
                'log_fh':     log_fh,
                'strategy':   strategy,
            }
        feat_info = (f"set#{params['feat_set']}"
                     if params.get('feat_set', -1) >= 0 else f"rand{params['n_features']}")
        _TAG_MAP = {
            'GA_feat':  'ğŸ”GA_feat ',   # ç‰¹å¾´é‡æ¢ç´¢
            'GA_param': 'ğŸ”§GA_param',   # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
            'GA_cross': 'ğŸ§¬GA_cross',   # äº¤å‰
            'random':   'ğŸ²Rnd     ',   # ãƒ©ãƒ³ãƒ€ãƒ 
        }
        tag = _TAG_MAP.get(strategy, f'?{strategy}')
        print(f"  [LAUNCH] è©¦è¡Œ#{trial_no:4d} {tag}  {params['arch']:12s}  "
              f"h={params['hidden']:4d}  feat={feat_info}  PID={proc.pid}")

    def poll_completed(self) -> list:
        """å®Œäº†/ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸè©¦è¡Œã®ãƒªã‚¹ãƒˆã‚’è¿”ã— running ã‹ã‚‰å‰Šé™¤"""
        done = []
        gpu_pids = get_gpu_compute_pids()   # ç¾åœ¨GPUä½¿ç”¨ä¸­ã®PIDã‚»ãƒƒãƒˆ
        now = time.time()

        with self.lock:
            for tno in list(self.running.keys()):
                info    = self.running[tno]
                elapsed = now - info['start_time']
                proc    = info['proc']

                if proc.poll() is None:   # ã¾ã å®Ÿè¡Œä¸­
                    pid = proc.pid

                    # â”€â”€ GPUä½¿ç”¨ä¸­PIDã®è¿½è·¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if pid in gpu_pids:
                        info['last_gpu_time'] = now   # GPUã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ™‚åˆ»ã‚’æ›´æ–°

                    last_gpu = info.get('last_gpu_time')
                    since_gpu = (now - last_gpu) if last_gpu else elapsed

                    # â”€â”€ GPUãƒãƒ¼ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¦ã‚©ãƒƒãƒãƒ‰ãƒƒã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # DATA_PREP_BUDGET ç§’ä»¥å†…ã¯ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­ã¨ã—ã¦è¨±å®¹
                    # ãã‚Œä»¥é™ã‚‚ GPU ã‚’ä½¿ã£ã¦ã„ãªã‘ã‚Œã°å¼·åˆ¶çµ‚äº†
                    if elapsed > DATA_PREP_BUDGET and since_gpu > NO_GPU_TIMEOUT:
                        print(f"  [NO-GPU] è©¦è¡Œ#{tno}  çµŒé{elapsed/60:.1f}åˆ†"
                              f"  GPUç„¡ä½¿ç”¨{since_gpu/60:.1f}åˆ† â†’ å¼·åˆ¶çµ‚äº†")
                        try:
                            proc.terminate()
                            proc.wait(timeout=10)
                        except Exception:
                            proc.kill()

                    # â”€â”€ å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    elif elapsed > TRIAL_TIMEOUT:
                        print(f"  [TIMEOUT] è©¦è¡Œ#{tno} ({elapsed/60:.0f}åˆ†è¶…) â†’ å¼·åˆ¶çµ‚äº†")
                        try:
                            proc.terminate()
                            proc.wait(timeout=10)
                        except Exception:
                            proc.kill()

                if proc.poll() is not None:
                    info['log_fh'].close()
                    done.append((tno, info))
                    del self.running[tno]
        return done

    def terminate_all(self):
        with self.lock:
            for info in self.running.values():
                try:
                    info['proc'].terminate()
                except Exception:
                    pass

    def __len__(self):
        return len(self.running)


# â”€â”€ å¸¸é§ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ« (ProcessPoolExecutor) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•ã‚³ã‚¹ãƒˆ (~8ç§’/è©¦è¡Œ) ã‚’åˆå›èµ·å‹•1å›ã«åœ§ç¸®ã™ã‚‹ã€‚
# workers stay alive â†’ Python/torch/CUDA init ã¯1ãƒ¯ãƒ¼ã‚«ãƒ¼å½“ãŸã‚Š1å›ã®ã¿ã€‚
class WorkerPool:
    """ProcessPoolExecutor ãƒ™ãƒ¼ã‚¹ã®å¸¸é§ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã€‚
    ParallelTrainer ã¨åŒä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æŒã¡å·®ã—æ›¿ãˆå¯èƒ½ã€‚
    """
    def __init__(self, max_workers: int, cache_pkl_path):
        import concurrent.futures as _cf
        import multiprocessing as _mp
        import sys as _sys
        _sys.path.insert(0, str(TRAIN_PY.parent))
        from train import worker_init  # noqa: å­˜åœ¨ç¢ºèª
        self._max_workers   = max_workers
        self._cache_path    = str(cache_pkl_path)
        self._futures: dict = {}   # trial_no -> Future
        self._meta:    dict = {}   # trial_no -> {params, strategy, start_time, trial_dir}
        self.lock           = threading.Lock()
        print(f"  [WorkerPool] {max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•ä¸­... (åˆå›ã®ã¿æ•°ç§’ã‹ã‹ã‚Šã¾ã™)")
        self._executor = _cf.ProcessPoolExecutor(
            max_workers=max_workers,
            mp_context=_mp.get_context('spawn'),
            initializer=_worker_init_proxy,
            initargs=(str(TRAIN_PY.parent), self._cache_path),
        )
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å…¨éƒ¨èµ·ã“ã—ã¦CUDAã‚’åˆæœŸåŒ–ã•ã›ã‚‹
        import concurrent.futures as _cf2
        warmup_futs = [self._executor.submit(_warmup_probe) for _ in range(max_workers)]
        _cf2.wait(warmup_futs, timeout=120)
        print(f"  [WorkerPool] å…¨{max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼æº–å‚™å®Œäº†")

    def _restart_executor(self) -> None:
        """BrokenProcessPool ç™ºç”Ÿæ™‚ã« executor ã‚’å†ä½œæˆã™ã‚‹"""
        import concurrent.futures as _cf
        import multiprocessing as _mp
        print(f"  [WorkerPool] executor å†èµ·å‹•ä¸­...")
        try:
            self._executor.shutdown(wait=False, cancel_futures=True)
        except Exception:
            pass
        # å£Šã‚ŒãŸ futures / meta ã‚’ç ´æ£„
        self._futures.clear()
        self._meta.clear()
        self._executor = _cf.ProcessPoolExecutor(
            max_workers=self._max_workers,
            mp_context=_mp.get_context('spawn'),
            initializer=_worker_init_proxy,
            initargs=(str(TRAIN_PY.parent), self._cache_path),
        )
        print(f"  [WorkerPool] executor å†èµ·å‹•å®Œäº† ({self._max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼)")

    def launch(self, trial_no: int, params: dict, best_pf: float, start_time: float,
               strategy: str = 'random'):
        trial_dir = TRIALS_DIR / f'trial_{trial_no:06d}'
        trial_dir.mkdir(parents=True, exist_ok=True)
        try:
            future = self._executor.submit(
                _run_trial_proxy,
                str(TRAIN_PY.parent),
                trial_no, params, str(trial_dir), best_pf, start_time,
            )
        except Exception as e:
            # BrokenProcessPool ã‚„ shutdown å¾Œã® submit â†’ executor ã‚’å†ä½œæˆã—ã¦å†è©¦è¡Œ
            print(f"  [WorkerPool] submitå¤±æ•— ({type(e).__name__}: {e}) â†’ executorå†èµ·å‹•")
            self._restart_executor()
            future = self._executor.submit(
                _run_trial_proxy,
                str(TRAIN_PY.parent),
                trial_no, params, str(trial_dir), best_pf, start_time,
            )
        with self.lock:
            self._futures[trial_no] = future
            self._meta[trial_no] = {
                'params':     params,
                'strategy':   strategy,
                'start_time': time.time(),
                'trial_dir':  trial_dir,
            }
        feat_info = (f"set#{params['feat_set']}"
                     if params.get('feat_set', -1) >= 0 else f"rand{params['n_features']}")
        _TAG_MAP = {
            'GA_feat':  'ğŸ”GA_feat ',
            'GA_param': 'ğŸ”§GA_param',
            'GA_cross': 'ğŸ§¬GA_cross',
            'random':   'ğŸ²Rnd     ',
        }
        tag = _TAG_MAP.get(strategy, f'?{strategy}')
        print(f"  [LAUNCH] è©¦è¡Œ#{trial_no:4d} {tag}  {params['arch']:12s}  "
              f"h={params['hidden']:4d}  feat={feat_info}")

    def poll_completed(self) -> list:
        done = []
        now  = time.time()
        broken = False
        with self.lock:
            for tno in list(self._futures.keys()):
                future = self._futures[tno]
                meta   = self._meta[tno]
                elapsed = now - meta['start_time']

                if future.done():
                    # future ãŒ BrokenProcessPool ã§çµ‚äº†ã—ã¦ã„ã‚‹ã‹ç¢ºèª
                    try:
                        exc = future.exception(timeout=0)
                        if exc is not None:
                            ename = type(exc).__name__
                            if 'BrokenProcessPool' in ename or 'broken' in str(exc).lower():
                                broken = True
                            print(f"  [WARN] è©¦è¡Œ#{tno} ä¾‹å¤–çµ‚äº†: {ename}: {exc}")
                    except Exception:
                        pass
                    done.append((tno, meta))
                    del self._futures[tno]
                    del self._meta[tno]
                elif elapsed > TRIAL_TIMEOUT:
                    future.cancel()
                    print(f"  [TIMEOUT] è©¦è¡Œ#{tno} ({elapsed/60:.0f}åˆ†è¶…) â†’ ã‚¹ã‚­ãƒƒãƒ—")
                    done.append((tno, meta))
                    del self._futures[tno]
                    del self._meta[tno]
        if broken:
            print(f"  [WorkerPool] BrokenProcessPool æ¤œå‡º â†’ executor å†èµ·å‹•")
            self._restart_executor()
        return done

    def terminate_all(self):
        with self.lock:
            for f in self._futures.values():
                f.cancel()
        try:
            self._executor.shutdown(wait=False, cancel_futures=True)
        except Exception:
            pass

    @property
    def running(self) -> dict:
        """write_progress ã¨ã®äº’æ›æ€§ã®ãŸã‚ _meta ã‚’ running ã¨ã—ã¦å…¬é–‹"""
        return self._meta

    def __len__(self):
        return len(self._futures)

    @property
    def n_active_workers(self) -> int:
        """å®Ÿéš›ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã§å®Ÿè¡Œä¸­ã®æ•° (ã‚­ãƒ¥ãƒ¼å¾…ã¡ã‚’é™¤ã)"""
        return min(len(self._futures), self._max_workers)


def _worker_init_proxy(train_py_dir: str, cache_pkl_path: str) -> None:
    """spawn ãƒ¯ãƒ¼ã‚«ãƒ¼ã®åˆæœŸåŒ– (pickleã§ãã‚‹é–¢æ•°ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„)"""
    import sys as _sys
    _sys.path.insert(0, train_py_dir)
    from train import worker_init
    worker_init(cache_pkl_path)


def _warmup_probe() -> bool:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒèµ·å‹•æ¸ˆã¿ã‹ç¢ºèªã™ã‚‹ã ã‘ã®ãƒ€ãƒŸãƒ¼ã‚¿ã‚¹ã‚¯"""
    return True


def _run_trial_proxy(train_py_dir: str, trial_no: int, params: dict,
                     trial_dir_str: str, best_pf: float, start_time: float) -> dict:
    """spawn ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ run_trial_worker ã‚’å‘¼ã¶ãƒ—ãƒ­ã‚­ã‚· (pickleã§ãã‚‹é–¢æ•°)"""
    import sys as _sys
    _sys.path.insert(0, train_py_dir)
    from train import run_trial_worker
    return run_trial_worker(trial_no, params, trial_dir_str, best_pf, start_time)


# â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»å¾©å…ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 mix ãƒ•ã‚©ãƒ«ãƒ€å…±æœ‰è¨­è¨ˆ:
#   å„ãƒãƒ¼ãƒ‰ã¯è‡ªåˆ†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚’æ›¸ãè¾¼ã‚€ â†’ ç«¶åˆã‚¼ãƒ­
#   mix/results_<NODE_ID>.json    : ã“ã®ãƒãƒ¼ãƒ‰ã®å…¨è©¦è¡Œçµæœ
#   mix/top100_<NODE_ID>/         : ã“ã®ãƒãƒ¼ãƒ‰ã® top100 ãƒ¢ãƒ‡ãƒ«
#   mix/best_<NODE_ID>/           : ã“ã®ãƒãƒ¼ãƒ‰ã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
#   mix/meta_<NODE_ID>.json       : ã“ã®ãƒãƒ¼ãƒ‰ã®ãƒ¡ã‚¿æƒ…å ±
#   èª­ã¿è¾¼ã¿æ™‚ã¯å…¨ãƒãƒ¼ãƒ‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ã¦çµ±åˆ top100 ã‚’å†æ§‹ç¯‰ã™ã‚‹

def save_checkpoint(results: list, best_pf: float) -> None:
    """è‡ªãƒãƒ¼ãƒ‰ã®çµæœã‚’ S3 mix/<NODE_ID>/* ã«ä¿å­˜ (ç«¶åˆãªã—)"""
    # è‡ªãƒãƒ¼ãƒ‰ã®çµæœã®ã¿ (node_id ãŒè‡ªåˆ† or node_id ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã—) ã‚’ä¿å­˜
    own = [r for r in results if r.get('node_id', NODE_ID) == NODE_ID]
    try:
        CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
        own_key = f'results_{NODE_ID}.json'
        tmp = CHECKPOINT_DIR / f'{own_key}.tmp'
        tmp.write_text(json.dumps(own, indent=2, ensure_ascii=False), encoding='utf-8')
        tmp.replace(CHECKPOINT_DIR / own_key)

        # best model ãƒ•ã‚¡ã‚¤ãƒ«
        for src, name in [(BEST_ONNX, f'best_{NODE_ID}/fx_model_best.onnx'),
                          (BEST_NORM, f'best_{NODE_ID}/norm_params_best.json'),
                          (BEST_JSON, f'best_{NODE_ID}/best_result.json')]:
            if src.exists():
                dst = CHECKPOINT_DIR / name
                dst.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src, dst)

        # top100 (è‡ªãƒãƒ¼ãƒ‰åˆ†ã®ã¿)
        top_dst = CHECKPOINT_DIR / f'top100_{NODE_ID}'
        if TOP_DIR.exists():
            if top_dst.exists():
                shutil.rmtree(top_dst)
            shutil.copytree(TOP_DIR, top_dst)

        meta = {'saved_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'node_id': NODE_ID, 'completed': len(own), 'best_pf': best_pf}
        meta_name = f'meta_{NODE_ID}.json'
        (CHECKPOINT_DIR / meta_name).write_text(
            json.dumps(meta, ensure_ascii=False), encoding='utf-8')
        print(f'  [CKPT] ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å®Œäº† node={NODE_ID} ({len(own)}ä»¶ / bestPF={best_pf:.4f})')

        if REMOTE_ENABLED():
            tag = 'GDrive' if GDRIVE_ENABLED else 'S3'
            # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ« (çµæœJSON / best / meta) ã¯åŒæœŸã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            ok = 0
            if remote_upload(CHECKPOINT_DIR / own_key, own_key): ok += 1
            if remote_upload(CHECKPOINT_DIR / meta_name, meta_name): ok += 1
            for name in [f'best_{NODE_ID}/fx_model_best.onnx',
                         f'best_{NODE_ID}/norm_params_best.json',
                         f'best_{NODE_ID}/best_result.json']:
                p = CHECKPOINT_DIR / name
                if p.exists() and remote_upload(p, name): ok += 1

            # top100 (å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«) ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å·®åˆ†ã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            # â†’ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
            def _upload_top100_bg(top_dst=top_dst):
                top100_ok = 0
                if not top_dst.exists():
                    return
                for f in top_dst.rglob('*'):
                    if not f.is_file():
                        continue
                    rel = f'top100_{NODE_ID}/{f.relative_to(top_dst)}'.replace('\\', '/')
                    if remote_upload(f, rel):
                        top100_ok += 1
                if top100_ok:
                    print(f'  [{tag}]  top100ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº† ({top100_ok}ä»¶)')

            import threading
            threading.Thread(target=_upload_top100_bg, daemon=True).start()
            print(f'  [{tag}]  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº† node={NODE_ID} ({ok}ä»¶ + top100:BG)')
        else:
            print(f'  [CKPT] ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœªè¨­å®š â†’ ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ä¿å­˜ ({CHECKPOINT_DIR})')
    except Exception as e:
        print(f'  [CKPT] ä¿å­˜å¤±æ•—: {e}')


def _merge_results_files(result_files: list[Path]) -> list:
    """è¤‡æ•°ãƒãƒ¼ãƒ‰ã® results_*.json ã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚¸ãƒ»é‡è¤‡æ’é™¤"""
    merged: dict = {}  # key: (node_id, trial) â†’ result
    for f in result_files:
        if not f.exists():
            continue
        try:
            data = json.loads(f.read_text(encoding='utf-8'))
            for r in data:
                nid = r.get('node_id', NODE_ID)
                tno = r.get('trial', 0)
                key = (nid, tno)
                if key not in merged or r.get('pf', 0) > merged[key].get('pf', 0):
                    merged[key] = r
        except Exception as e:
            print(f'  [WARN] çµæœãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼å¤±æ•— {f}: {e}')
    return sorted(merged.values(), key=lambda x: x.get('trial', 0))


def fetch_other_nodes_results() -> list:
    """ãƒªãƒ¢ãƒ¼ãƒˆã‹ã‚‰ä»–ãƒãƒ¼ãƒ‰ã®çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è¿”ã™ (ãƒãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ç”¨)"""
    if not REMOTE_ENABLED():
        return []
    other_files = remote_list_node_keys('results_')
    results_all = []
    for rel_key in other_files:
        if f'results_{NODE_ID}.json' == rel_key:
            continue
        local = CHECKPOINT_DIR / rel_key
        if remote_download(rel_key, local):
            try:
                data = json.loads(local.read_text(encoding='utf-8'))
                results_all.extend(data)
            except Exception:
                pass
    return results_all


def restore_checkpoint() -> bool:
    """S3 mix/ ã‹ã‚‰å…¨ãƒãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒãƒ¼ã‚¸å¾©å…ƒ"""
    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)

    if REMOTE_ENABLED():
        tag = 'GDrive' if GDRIVE_ENABLED else 'S3'
        print(f'  [{tag}]  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèªä¸­ ...')
        # å…¨ãƒãƒ¼ãƒ‰ã® results_*.json ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        result_keys = remote_list_node_keys('results_')
        if not result_keys:
            print(f'  [{tag}]  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãªã— (å…¨ãƒãƒ¼ãƒ‰)')
        else:
            for rk in result_keys:
                remote_download(rk, CHECKPOINT_DIR / rk)
                print(f'  [{tag}]  å–å¾—: {rk}')
        # å…¨ãƒãƒ¼ãƒ‰ã® meta_*.json ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        for mk in remote_list_node_keys('meta_'):
            remote_download(mk, CHECKPOINT_DIR / mk)
        # ã“ã®ãƒãƒ¼ãƒ‰ + ä»–ãƒãƒ¼ãƒ‰ã® best ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«)
        for bk in remote_list_best_keys():   # ä¾‹: best_h100/fx_model_best.onnx
            remote_download(bk, CHECKPOINT_DIR / bk)
        # å…¨ãƒãƒ¼ãƒ‰ã® top100 ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ONNXå«ã‚€å…¨ãƒ•ã‚¡ã‚¤ãƒ«)
        top100_count = 0
        for rel in remote_list_top100_keys():   # ä¾‹: top100_h100/rank_001/fx_model.onnx
            dest = CHECKPOINT_DIR / rel
            dest.parent.mkdir(parents=True, exist_ok=True)
            if remote_download(rel, dest):
                top100_count += 1
        if top100_count:
            print(f'  [{tag}]  top100 {top100_count}ãƒ•ã‚¡ã‚¤ãƒ«å–å¾— (å…¨ãƒãƒ¼ãƒ‰)')

    # â”€â”€ å…¨ãƒãƒ¼ãƒ‰ã® results_*.json ã‚’ãƒãƒ¼ã‚¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    result_files = list(CHECKPOINT_DIR.glob('results_*.json'))
    if not result_files:
        return False
    try:
        merged = _merge_results_files(result_files)
        if not merged:
            return False
        # ãƒãƒ¼ãƒ‰IDä»˜ä¸ (å¤ã„ãƒ‡ãƒ¼ã‚¿ã§æ¬ è½ã—ã¦ã„ã‚‹å ´åˆã®è£œå®Œ)
        own_key = f'results_{NODE_ID}.json'
        for r in merged:
            if 'node_id' not in r:
                # ã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¥ãŸã‹ç‰¹å®š
                r['node_id'] = NODE_ID  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        best_pf_all = max((r.get('pf', 0) for r in merged if r.get('trades', 0) >= 200), default=0.0)
        print(f'  [CKPT] å…¨ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸: {len(merged)}ä»¶  bestPF={best_pf_all:.4f}  '
              f'ãƒãƒ¼ãƒ‰: {sorted({r.get("node_id","?") for r in merged})}')
        tmp = ALL_RESULTS.with_suffix('.tmp')
        tmp.write_text(json.dumps(merged, indent=2, ensure_ascii=False), encoding='utf-8')
        tmp.replace(ALL_RESULTS)

        # ã“ã®ãƒãƒ¼ãƒ‰ã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
        best_dir = CHECKPOINT_DIR / f'best_{NODE_ID}'
        if best_dir.exists():
            for src_name, dst in [('fx_model_best.onnx',   BEST_ONNX),
                                   ('norm_params_best.json', BEST_NORM),
                                   ('best_result.json',       BEST_JSON)]:
                src = best_dir / src_name
                if src.exists():
                    shutil.copy2(src, dst)
        else:
            # æ—§å½¢å¼ (node_id ãªã—) ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            for src_name, dst in [('fx_model_best.onnx',   BEST_ONNX),
                                   ('norm_params_best.json', BEST_NORM),
                                   ('best_result.json',       BEST_JSON)]:
                src = CHECKPOINT_DIR / src_name
                if src.exists():
                    shutil.copy2(src, dst)

        # top100 ã‚’å…¨ãƒãƒ¼ãƒ‰åˆ†ã¾ã¨ã‚ã¦åˆæˆ â†’ TOP_DIR ã«å±•é–‹
        _restore_merged_top100()
        print('  [CKPT] å¾©å…ƒå®Œäº† â†’ å…¨ãƒãƒ¼ãƒ‰ã®çµæœã‚’çµ±åˆã—ã¦å†é–‹ã—ã¾ã™')

        # å¾©å…ƒå¾Œã«ç‰¹å¾´é‡é‡è¦åº¦ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ
        def _run_backfill():
            import importlib.util, time as _t
            _t.sleep(2)
            try:
                spec = importlib.util.spec_from_file_location(
                    'backfill_top100', str(OUT_DIR / 'backfill_top100.py'))
                mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(mod)
                mod.main()
            except Exception as _e:
                print(f'  [BACKFILL] ã‚¨ãƒ©ãƒ¼: {_e}')
        threading.Thread(target=_run_backfill, daemon=True).start()

        return True
    except Exception as e:
        print(f'  [CKPT] å¾©å…ƒå¤±æ•—: {e}')
        return False


def _restore_merged_top100() -> None:
    """å…¨ãƒãƒ¼ãƒ‰ã® top100_<NODE_ID>/ ã‚’ãƒãƒ¼ã‚¸ã—ã¦ TOP_DIR ã«å±•é–‹"""
    # ãƒãƒ¼ãƒ‰ã”ã¨ã® top100 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åé›†
    top_dirs = [d for d in CHECKPOINT_DIR.iterdir()
                if d.is_dir() and d.name.startswith('top100_')]
    # æ—§å½¢å¼ (top100/) ã‚‚è€ƒæ…®
    legacy = CHECKPOINT_DIR / 'top100'
    if legacy.exists() and legacy.is_dir():
        top_dirs.append(legacy)
    if not top_dirs:
        return

    # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ PF é™é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ TOP_N ã‚’ TOP_DIR ã«å±•é–‹
    all_models: list[tuple[float, Path]] = []
    for td in top_dirs:
        for rank_dir in td.iterdir():
            if not rank_dir.is_dir():
                continue
            rf = rank_dir / 'result.json'
            if rf.exists():
                try:
                    r = json.loads(rf.read_text(encoding='utf-8'))
                    all_models.append((r.get('pf', 0), rank_dir))
                except Exception:
                    pass
    all_models.sort(key=lambda x: -x[0])

    TOP_DIR.mkdir(parents=True, exist_ok=True)
    TOP_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    cache_restored = 0
    for rank_i, (pf, src_dir) in enumerate(all_models[:TOP_N], 1):
        rank_dst = TOP_DIR / f'rank_{rank_i:03d}'
        if rank_dst.exists():
            shutil.rmtree(rank_dst)
        shutil.copytree(src_dir, rank_dst)
        # top_cache ã«ã‚‚ trial ç•ªå·ã§ã‚³ãƒ”ãƒ¼
        rf = rank_dst / 'result.json'
        if rf.exists():
            try:
                r = json.loads(rf.read_text(encoding='utf-8'))
                tno = r.get('trial', 0)
                nid = r.get('node_id', NODE_ID)
                if tno > 0:
                    cache_key = f'trial_{nid}_{tno:06d}'
                    cache_dst = TOP_CACHE_DIR / cache_key
                    if not cache_dst.exists():
                        cache_dst.mkdir(parents=True, exist_ok=True)
                        for fn in ['fx_model.onnx', 'norm_params.json', 'report.html']:
                            fs = rank_dst / fn
                            if fs.exists():
                                shutil.copy2(fs, cache_dst / fn)
                        cache_restored += 1
            except Exception:
                pass
    if cache_restored:
        print(f'  [CKPT] top_cache ã« {cache_restored}ä»¶ å¾©å…ƒ (å…¨ãƒãƒ¼ãƒ‰åˆç®—)')


# â”€â”€ ãƒ¡ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _precache_data() -> bool:
    """ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ä½œæˆã—ã¦å…¨è©¦è¡ŒãŒå³åº§ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹"""
    import pickle
    DATA_PATH = Path(os.environ.get('DATA_PATH', '/workspace/data/USDJPY_M1.csv'))
    cache_path = TRIALS_DIR.parent / 'df_cache_H1.pkl'
    if cache_path.exists():
        print(f"  [PRE-CACHE] ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ—¢å­˜: {cache_path}")
        return True
    if not DATA_PATH.exists():
        print(f"  [PRE-CACHE] ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {DATA_PATH}")
        return False
    print(f"  [PRE-CACHE] ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ä½œæˆä¸­... (åˆå›ã®ã¿æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)")
    try:
        import sys as _sys
        _sys.path.insert(0, str(TRAIN_PY.parent))
        from features import load_data, add_indicators
        import numpy as np
        from datetime import timedelta
        t0 = time.time()
        df = load_data(str(DATA_PATH), timeframe='H1')
        df = add_indicators(df)
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(inplace=True)
        test_start = df.index[-1] - timedelta(days=365)
        df_tr = df[df.index < test_start].copy()
        df_te = df[df.index >= test_start].copy()
        tmp = cache_path.with_suffix('.tmp')
        with open(tmp, 'wb') as f:
            pickle.dump((df_tr, df_te), f)
        tmp.replace(cache_path)
        print(f"  [PRE-CACHE] å®Œäº† {time.time()-t0:.1f}ç§’  "
              f"è¨“ç·´:{len(df_tr):,}è¡Œ  ãƒ†ã‚¹ãƒˆ:{len(df_te):,}è¡Œ  â†’ {cache_path}")
        return True
    except Exception as e:
        print(f"  [PRE-CACHE] å¤±æ•— (è¨“ç·´ã¯ç¶šè¡Œ): {e}")
        return False


def main():
    global _mode_start_time, _important_features

    # SIGTERM (ã‚³ãƒ³ãƒ†ãƒŠåœæ­¢æ™‚) ã‚’å—ã‘å–ã£ãŸã‚‰ stop.flag ã‚’ç½®ã„ã¦graceful shutdown
    def _sigterm_handler(signum, frame):
        print('\n[SIGNAL] SIGTERM å—ä¿¡ â†’ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã—ã¦åœæ­¢ã—ã¾ã™...')
        STOP_FLAG.touch()
    signal.signal(signal.SIGTERM, _sigterm_handler)
    signal.signal(signal.SIGINT,  _sigterm_handler)

    TRIALS_DIR.mkdir(parents=True, exist_ok=True)
    TOP_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    TOP_DIR.mkdir(parents=True, exist_ok=True)

    gpu_name = NODE_ID.upper()
    print('=' * 60)
    storage_tag = 'GDrive' if GDRIVE_ENABLED else ('S3' if S3_ENABLED else 'ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿')
    print(f'FX AI EA v8 - ä¸¦åˆ—ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ  ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: {storage_tag}/{NODE_ID}')
    print(f'  GPU     : {gpu_name}  ({_GPU_VRAM_GB:.0f} GB)  tier={_GPU_TIER}')
    print(f'  ä¸¦åˆ—æ•°  : {MAX_PARALLEL}  VRAM/è©¦è¡Œ={VRAM_PER_TRIAL} GB  H100_MODE={H100_MODE}')
    print(f'  ãƒ¢ãƒ‡ãƒ«  : hidden={[v for v in HIDDEN_MAP.get("mlp",[])]}  batch={BATCH_CHOICES}')
    print(f'  TOP {TOP_N} ä¿å­˜  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ {TRIAL_TIMEOUT//60}åˆ†  stop.flag: {STOP_FLAG}')
    print('=' * 60)

    # â”€â”€ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ¥ç¶šç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f'  GDRIVE_ENABLED: {GDRIVE_ENABLED}')
    print(f'  S3_ENABLED    : {S3_ENABLED}  (S3_ENDPOINT: {S3_ENDPOINT or "(æœªè¨­å®š)"})')
    if GDRIVE_ENABLED:
        _gdrive.test_connection()
    elif S3_ENABLED:
        try:
            cl = _s3_client()
            cl.put_object(Bucket=S3_BUCKET, Key=f'{S3_PREFIX}/.ping', Body=b'ok')
            cl.delete_object(Bucket=S3_BUCKET, Key=f'{S3_PREFIX}/.ping')
            print('  [S3] æ¥ç¶šãƒ†ã‚¹ãƒˆ OK âœ…')
        except Exception as e:
            print(f'  [S3] æ¥ç¶šãƒ†ã‚¹ãƒˆ å¤±æ•— âŒ: {e}')
    else:
        print('  [ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸] æœªè¨­å®š â†’ ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ (GDRIVE_FOLDER_ID + GDRIVE_CREDENTIALS_BASE64 ã‚’è¨­å®š)')

    # â”€â”€ èµ·å‹•æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ä½œæˆ (å…¨è©¦è¡ŒãŒå³åº§ã«å­¦ç¿’é–‹å§‹ã§ãã‚‹) â”€â”€
    _precache_data()

    if STOP_FLAG.exists():
        STOP_FLAG.unlink()

    rng     = random.Random()
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # WorkerPool (ProcessPoolExecutor) vs ParallelTrainer (subprocess.Popen)
    #
    # Windows ã§ã¯ ProcessPoolExecutor ã® future.cancel() ã¯å®Ÿè¡Œä¸­ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’
    # æ­¢ã‚ã‚‰ã‚Œãªã„ (Linux ã¨ç•°ãªã‚Š SIGKILL ãŒå±Šã‹ãªã„)ã€‚
    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡ºå¾Œã‚‚ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒã‚¹ãƒ­ãƒƒãƒˆã‚’å æœ‰ã—ç¶šã‘ã‚‹ãŸã‚ã€
    # æ–°è¦è©¦è¡ŒãŒã‚­ãƒ¥ãƒ¼è©°ã¾ã‚Š â†’ å®Œäº†æ•°ãŒæ­¢ã¾ã‚‹ (stuck-at-N ç—‡çŠ¶)ã€‚
    # â†’ Windows ã§ã¯ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ¼ãƒ‰ (ParallelTrainer) ã‚’å¼·åˆ¶ä½¿ç”¨ã™ã‚‹ã€‚
    # Linux/Docker ç’°å¢ƒã§ã¯ WorkerPool ã®èµ·å‹•ã‚³ã‚¹ãƒˆå‰Šæ¸›ãƒ¡ãƒªãƒƒãƒˆã‚’æ´»ã‹ã™ã€‚
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _cache_pkl  = TRIALS_DIR.parent / 'df_cache_H1.pkl'
    _on_windows = platform.system() == 'Windows'
    if _cache_pkl.exists() and not _on_windows:
        try:
            trainer = WorkerPool(MAX_PARALLEL, _cache_pkl)
        except Exception as _e:
            print(f"  [WARN] WorkerPool åˆæœŸåŒ–å¤±æ•— â†’ subprocess ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {_e}")
            trainer = ParallelTrainer()
    else:
        if _on_windows:
            print("  [INFO] Windows ç’°å¢ƒ â†’ ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ¼ãƒ‰ä½¿ç”¨ "
                  "(WorkerPool ã¯ Linux/Docker å°‚ç”¨: proc.terminate() ã§ç¢ºå®Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)")
        else:
            print("  [INFO] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã— â†’ subprocess ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•")
        trainer = ParallelTrainer()
    results  = []
    best_pf  = 0.0
    trial_no = 1
    start    = time.time()
    _mode_start_time = start   # 10åˆ†ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆã®åŸºæº–æ™‚åˆ»

    # â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ (ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚¦ãƒ³ãƒˆæ™‚ã¯è‡ªå‹•ç¶™ç¶š) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not ALL_RESULTS.exists():
        restore_checkpoint()

    # â”€â”€ ä»–ãƒãƒ¼ãƒ‰çµæœã®å®šæœŸãƒãƒ¼ã‚¸ã‚¹ãƒ¬ãƒƒãƒ‰ (5åˆ†ã”ã¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _other_merge_stop = threading.Event()
    def _other_nodes_merge_loop():
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ä»–ãƒãƒ¼ãƒ‰ã®æ–°ç€çµæœã‚’å–ã‚Šè¾¼ã‚“ã§ãƒãƒ¼ã‚¸"""
        while not _other_merge_stop.wait(300):   # 5åˆ†ã”ã¨
            if not REMOTE_ENABLED():
                continue
            try:
                other = fetch_other_nodes_results()
                if not other:
                    continue
                new_count = 0
                for r in other:
                    nid  = r.get('node_id', '?')
                    tno  = r.get('trial', 0)
                    key  = (nid, tno)
                    if not any(x.get('node_id') == nid and x.get('trial') == tno
                               for x in results):
                        results.append(r)
                        new_count += 1
                if new_count:
                    results.sort(key=lambda x: x.get('trial', 0))
                    print(f'  [SYNC] ä»–ãƒãƒ¼ãƒ‰çµæœã‚’ {new_count}ä»¶ å–ã‚Šè¾¼ã¿ '
                          f'(åˆè¨ˆ {len(results)}ä»¶)')
                    try:
                        rebuild_top_n(results)
                    except Exception:
                        pass
            except Exception as e:
                print(f'  [SYNC] ä»–ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸å¤±æ•—: {e}')
    _sync_thread = threading.Thread(target=_other_nodes_merge_loop, daemon=True, name='NodeSync')
    _sync_thread.start()

    # æ—¢å­˜çµæœã‚’å¼•ãç¶™ã
    if ALL_RESULTS.exists():
        try:
            raw = json.loads(ALL_RESULTS.read_text(encoding='utf-8'))
            # â”€â”€ é‡è¤‡æ’é™¤: (node_id, trial) ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            seen_key: set = set()
            results = []
            for r in raw:
                nid_r = r.get('node_id', NODE_ID)
                tno_r = r.get('trial', -1)
                key_r = (nid_r, tno_r)
                if key_r not in seen_key:
                    seen_key.add(key_r)
                    results.append(r)

            if len(raw) != len(results):
                print(f"  [DEDUP] é‡è¤‡é™¤å»: {len(raw)} â†’ {len(results)} ä»¶")
                tmp = ALL_RESULTS.with_suffix('.tmp')
                tmp.write_text(json.dumps(results, indent=2, ensure_ascii=False),
                               encoding='utf-8')
                tmp.replace(ALL_RESULTS)
            # è‡ªãƒãƒ¼ãƒ‰ã®æœ€å¤§ trial ç•ªå·ã‹ã‚‰æ¬¡ã® trial ç•ªå·ã‚’æ±ºå®š
            own_trials = [r.get('trial', 0) for r in results
                          if r.get('node_id', NODE_ID) == NODE_ID]
            trial_no = max(own_trials, default=0) + 1
            valid    = [r for r in results if r.get('pf', 0) > 0]
            if valid:
                best_r  = max(valid, key=lambda r: r['pf'])
                best_pf = best_r['pf']
                print(f"  å‰å›æœ€è‰¯PF={best_pf:.4f}  å®Œäº†{len(results)}ä»¶  æ¬¡è©¦è¡Œ#{trial_no}")
        except Exception:
            pass

    last_checkpoint        = time.time()
    completed_since_ckpt   = 0   # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾Œã®å®Œäº†ä»¶æ•°ã‚«ã‚¦ãƒ³ã‚¿

    write_progress(trainer.running, results, best_pf, start)

    # â”€â”€ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _loop_errors = 0
    while True:
      try:
        # stop.flag ãƒã‚§ãƒƒã‚¯
        if STOP_FLAG.exists():
            print(f"\n[STOP] stop.flag æ¤œå‡º â†’ å®Ÿè¡Œä¸­ã®è©¦è¡Œã‚’å¾…æ©Ÿã—ã¦çµ‚äº†")
            trainer.terminate_all()
            break

        # â”€â”€ å®Œäº†ã—ãŸè©¦è¡Œã‚’å›å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        newly_done = trainer.poll_completed()
        completed_since_ckpt += len(newly_done)
        for tno, info in newly_done:
            result_path = info['trial_dir'] / 'last_result.json'
            r = {}
            if result_path.exists():
                try:
                    r = json.loads(result_path.read_text(encoding='utf-8'))
                except Exception:
                    pass

            pf     = float(r.get('pf', 0.0))
            trades = int(r.get('trades', 0))
            sr     = float(r.get('sr', 0.0))
            max_dd = float(r.get('max_dd', 0.0))
            elapsed= round(time.time() - info['start_time'], 0)

            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿å±•é–‹ (trial/pf ç­‰ã®çµæœãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–ã—ã¦ä¸Šæ›¸ãã‚’é˜²ã)
            _meta_keys = {'trial', 'pf', 'trades', 'sr', 'max_dd', 'win_rate',
                          'net_pnl', 'gross_profit', 'gross_loss', 'elapsed_sec',
                          'timestamp', 'strategy', 'node_id'}
            record = {
                **{k: v for k, v in info['params'].items() if k not in _meta_keys},
                'trial':     tno,
                'node_id':   NODE_ID,          # ãƒãƒ¼ãƒ‰ID (ãƒãƒ¼ã‚¸æ™‚ã®è­˜åˆ¥å­)
                'gpu_name':  GPU_NAME,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'strategy':  info.get('strategy', 'random'),
                'pf':        pf,
                'trades':    trades,
                'win_rate':  r.get('win_rate',      0.0),
                'net_pnl':   r.get('net_pnl',       0.0),
                'gross_profit': r.get('gross_profit', 0.0),
                'gross_loss':   r.get('gross_loss',   0.0),
                'sr':        sr,
                'max_dd':    max_dd,
                'elapsed_sec': elapsed,
                'feature_importance': r.get('feature_importance', []),
            }
            # é‡è¤‡é˜²æ­¢: åŒã˜ (node_id, trial) ãŒã™ã§ã«ã‚ã‚Œã°ä¸Šæ›¸ãã€ãªã‘ã‚Œã°è¿½åŠ 
            existing_idx = next((i for i, r in enumerate(results)
                                 if r['trial'] == tno and r.get('node_id', NODE_ID) == NODE_ID), None)
            if existing_idx is not None:
                results[existing_idx] = record
            else:
                results.append(record)
            results.sort(key=lambda x: x['trial'])

            # all_results.json ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿
            try:
                tmp = ALL_RESULTS.with_suffix('.tmp')
                tmp.write_text(json.dumps(results, indent=2, ensure_ascii=False),
                               encoding='utf-8')
                tmp.replace(ALL_RESULTS)
            except Exception as e:
                print(f"  [WARN] çµæœä¿å­˜å¤±æ•—: {e}")

            # TOP_N ã«å…¥ã£ãŸã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦å†æ§‹ç¯‰
            if pf > 0 and trades >= 200:
                try:
                    save_trial_model(tno)
                    rebuild_top_n(results)
                except Exception as e:
                    print(f"  [WARN] TOP{TOP_N} æ›´æ–°å¤±æ•—: {e}")

            # é‡è¦ç‰¹å¾´é‡ãƒ—ãƒ¼ãƒ«ã‚’æ›´æ–° (5ä»¶ã”ã¨)
            if len(results) % 5 == 0:
                try:
                    _update_important_features(results)
                    if _important_features:
                        print(f"  [IMP] é‡è¦ç‰¹å¾´é‡TOP5: {_important_features[:5]}")
                except Exception as e:
                    print(f"  [WARN] é‡è¦ç‰¹å¾´é‡æ›´æ–°å¤±æ•—: {e}")

            # 10åˆ†ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒ­ã‚°
            if _mode_start_time > 0:
                elapsed_total = time.time() - _mode_start_time
                cycle   = int(elapsed_total // MODE_SWITCH_SEC)
                remain  = int(MODE_SWITCH_SEC - (elapsed_total % MODE_SWITCH_SEC))
                if remain <= 5:   # åˆ‡ã‚Šæ›¿ãˆç›´å‰ã«é€šçŸ¥
                    next_mode = 'GA' if cycle % 2 == 0 else 'ãƒ©ãƒ³ãƒ€ãƒ '
                    print(f"  [MODE] ã¾ã‚‚ãªã {next_mode} ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ...")

            # ãƒ™ã‚¹ãƒˆæ›´æ–° (200å–å¼•ä»¥ä¸Šã®ã¿å¯¾è±¡)
            if pf > best_pf and trades >= 200:
                best_pf = pf
                for src, dst in [(info['trial_dir'] / 'fx_model.onnx',    BEST_ONNX),
                                  (info['trial_dir'] / 'norm_params.json', BEST_NORM)]:
                    if src.exists():
                        shutil.copy2(src, dst)
                BEST_JSON.write_text(
                    json.dumps({**info['params'], 'pf': best_pf,
                                'sr': sr, 'max_dd': max_dd, 'trial': tno},
                               indent=2, ensure_ascii=False), encoding='utf-8')
                # ãƒ™ã‚¹ãƒˆæ›´æ–°æ™‚ã«é‡è¦ç‰¹å¾´é‡ãƒ—ãƒ¼ãƒ«ã‚‚å³æ™‚æ›´æ–°
                try:
                    _update_important_features(results)
                    if _important_features:
                        print(f"  [IMP] ãƒ™ã‚¹ãƒˆæ›´æ–° â†’ é‡è¦ç‰¹å¾´é‡æ›´æ–°: {_important_features[:5]}")
                except Exception:
                    pass
                print(f"  [BEST] è©¦è¡Œ#{tno}  PF={pf:.4f}  SR={sr:.3f}  MaxDD={max_dd:.4f}")
            else:
                print(f"  [DONE] è©¦è¡Œ#{tno:4d}  PF={pf:.4f}  SR={sr:.3f}  "
                      f"MaxDD={max_dd:.4f}  å–å¼•={trades}  "
                      f"{elapsed/60:.1f}åˆ†  (ãƒ™ã‚¹ãƒˆ={best_pf:.4f})")

        # â”€â”€ æ–°è¦è©¦è¡Œã‚’æŠ•å…¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        n_active = trainer.n_active_workers if isinstance(trainer, WorkerPool) else len(trainer)
        max_par = get_max_parallel(n_active)
        # WorkerPool (ProcessPoolExecutor) ã®å ´åˆã¯ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°:
        # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°Ã—2 ã‚’ã‚­ãƒ¥ãƒ¼ã«ä¿æŒ â†’ ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒçµ‚ã‚ã£ãŸç¬é–“ã«æ¬¡ã‚¸ãƒ§ãƒ–é–‹å§‹
        submit_limit = max_par * 2 if isinstance(trainer, WorkerPool) else max_par
        while len(trainer) < submit_limit:
            if STOP_FLAG.exists():
                break
            try:
                p, strategy = next_params(results, rng)
            except Exception as _np_exc:
                print(f"  [WARN] next_params å¤±æ•—: {_np_exc} â†’ ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                p, strategy = sample_params(rng), 'random'
            try:
                trainer.launch(trial_no, p, best_pf, start, strategy)
            except Exception as _launch_exc:
                print(f"  [WARN] launch å¤±æ•—: {_launch_exc} â†’ ã‚¹ã‚­ãƒƒãƒ—")
                time.sleep(5)
                break
            trial_no += 1
            if isinstance(trainer, WorkerPool):
                time.sleep(0.1)   # ã‚­ãƒ¥ãƒ¼æŠ•å…¥ã¯é«˜é€Ÿã§OK (CUDAåˆæœŸåŒ–æ¸ˆã¿)
            else:
                time.sleep(LAUNCH_INTERVAL)

        # â”€â”€ é€²æ— JSON æ›¸ãè¾¼ã¿ (5ç§’ã”ã¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        write_progress(trainer.running, results, best_pf, start)

        # â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: 10è©¦è¡Œã”ã¨ or 10åˆ†ã”ã¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        should_ckpt = (completed_since_ckpt >= CHECKPOINT_EVERY_N or
                       time.time() - last_checkpoint >= CHECKPOINT_INTERVAL)
        if should_ckpt:
            save_checkpoint(results, best_pf)
            last_checkpoint      = time.time()
            completed_since_ckpt = 0

        time.sleep(1 if isinstance(trainer, WorkerPool) else 5)
        _loop_errors = 0  # æ­£å¸¸ãƒ«ãƒ¼ãƒ—ãŒå›ã‚Œã°ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ

      except KeyboardInterrupt:
          print(f"\n[STOP] KeyboardInterrupt â†’ çµ‚äº†å‡¦ç†")
          trainer.terminate_all()
          break
      except Exception as _loop_exc:
          import traceback as _tb
          _loop_errors += 1
          print(f"  [ERROR] ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ä¾‹å¤– ({_loop_errors}å›ç›®): "
                f"{type(_loop_exc).__name__}: {_loop_exc}")
          _tb.print_exc()
          if _loop_errors >= 10:
              print(f"  [ERROR] é€£ç¶šã‚¨ãƒ©ãƒ¼10å› â†’ çµ‚äº†")
              trainer.terminate_all()
              break
          time.sleep(5)
          continue

    # â”€â”€ çµ‚äº†å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    write_progress({}, results, best_pf, start)
    save_checkpoint(results, best_pf)   # åœæ­¢æ™‚ã«å¿…ãšãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    print(f"\nå®Œäº†  ç·è©¦è¡Œ: {len(results)}ä»¶  æœ€è‰¯PF: {best_pf:.4f}")
    if BEST_ONNX.exists():
        shutil.copy2(BEST_ONNX, OUT_DIR / 'fx_model.onnx')
    if BEST_NORM.exists():
        shutil.copy2(BEST_NORM, OUT_DIR / 'norm_params.json')

    # â”€â”€ MT5 Common\Files ã¸è‡ªå‹•ã‚³ãƒ”ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _appdata = Path(os.environ.get('APPDATA', ''))
    _common  = _appdata / 'MetaQuotes' / 'Terminal' / 'Common' / 'Files'
    if _common.exists():
        _copies = [
            (OUT_DIR / 'fx_model.onnx',    _common / 'fx_model.onnx'),
            (OUT_DIR / 'norm_params.json',  _common / 'norm_params.json'),
        ]
        for src, dst in _copies:
            if src.exists():
                shutil.copy2(src, dst)
                print(f"  â†’ Common\\Files\\ ã«ã‚³ãƒ”ãƒ¼: {src.name}")
    else:
        print(f"  [skip] Common\\Files æœªæ¤œå‡º: {_common}")


if __name__ == '__main__':
    main()
