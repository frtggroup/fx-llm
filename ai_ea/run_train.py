"""
FX AI EA è‡ªå‹•ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° v8 - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
  ãƒ»æœ€åˆã® 500 ä»¶: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ (æ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º)
  ãƒ»501 ä»¶ä»¥é™: 75% éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (TOP çµæœã‚’äº¤å‰ãƒ»çªç„¶å¤‰ç•°) + 25% ãƒ©ãƒ³ãƒ€ãƒ 
  ãƒ»VRAM / GPU ä½¿ç”¨ç‡ã‚’ç›£è¦–ã—ã¦å‹•çš„ã«ä¸¦åˆ—æ•°ã‚’æ±ºå®š
  ãƒ»åœæ­¢æ¡ä»¶ãªã— (stop.flag ãŒç½®ã‹ã‚Œã‚‹ã¾ã§ç„¡é™ç¶™ç¶š)
  ãƒ»TOP100 ãƒ¢ãƒ‡ãƒ«ä¿å­˜ + SR / DD / è³‡ç”£æ›²ç·šãƒ¬ãƒãƒ¼ãƒˆ
"""
import os, subprocess, sys, json, shutil, time, random, threading, signal
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))
from feature_sets import FEATURE_SETS

PY        = sys.executable
TRAIN_PY  = Path(__file__).parent / 'train.py'
OUT_DIR   = Path(__file__).parent

_WORKSPACE    = Path('/workspace') if Path('/workspace').exists() else OUT_DIR.parent
STOP_FLAG     = _WORKSPACE / 'stop.flag'
TRIALS_DIR    = OUT_DIR / 'trials'
TOP_CACHE_DIR = OUT_DIR / 'top_cache'
TOP_DIR       = OUT_DIR / 'top100'
ALL_RESULTS   = OUT_DIR / 'all_results.json'
PROGRESS_JSON = OUT_DIR / 'progress.json'
BEST_ONNX     = OUT_DIR / 'fx_model_best.onnx'
BEST_NORM     = OUT_DIR / 'norm_params_best.json'
BEST_JSON     = OUT_DIR / 'best_result.json'

# â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ (åœæ­¢â†’å†é–‹ç”¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ­ãƒ¼ã‚«ãƒ«: /workspace/data/checkpoint/ ã«å®šæœŸä¿å­˜
# S3: ç’°å¢ƒå¤‰æ•° S3_* ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã° Sakura ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚‚ä¿å­˜
CHECKPOINT_DIR      = _WORKSPACE / 'data' / 'checkpoint'
CHECKPOINT_INTERVAL = 600   # ç§’ (10åˆ†ã”ã¨ã«ä¿å­˜)
CHECKPOINT_EVERY_N  = 10    # ä»¶ (10è©¦è¡Œå®Œäº†ã”ã¨ã«ä¿å­˜)

S3_ENDPOINT  = os.environ.get('S3_ENDPOINT',   '')   # ä¾‹: https://s3.isk01.sakurastorage.jp
S3_ACCESS_KEY= os.environ.get('S3_ACCESS_KEY',  '')
S3_SECRET_KEY= os.environ.get('S3_SECRET_KEY',  '')
S3_BUCKET    = os.environ.get('S3_BUCKET',      'fxea')
S3_PREFIX    = os.environ.get('S3_PREFIX',      'checkpoint')
S3_ENABLED   = bool(S3_ENDPOINT and S3_ACCESS_KEY and S3_SECRET_KEY)


def _s3_client():
    import boto3
    return boto3.client(
        's3',
        endpoint_url      = S3_ENDPOINT,
        aws_access_key_id = S3_ACCESS_KEY,
        aws_secret_access_key = S3_SECRET_KEY,
        region_name       = os.environ.get('S3_REGION', 'jp-north-1'),
    )


def s3_upload(local_path: Path, s3_key: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ S3 ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã‚’æŠ•ã’ãš False ã‚’è¿”ã™"""
    try:
        _s3_client().upload_file(str(local_path), S3_BUCKET,
                                 f'{S3_PREFIX}/{s3_key}')
        return True
    except Exception as e:
        print(f'  [S3] uploadå¤±æ•— {s3_key}: {e}')
        return False


def s3_download(s3_key: str, local_path: Path) -> bool:
    """S3 ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚å¤±æ•—ã—ãŸã‚‰ False ã‚’è¿”ã™"""
    try:
        local_path.parent.mkdir(parents=True, exist_ok=True)
        _s3_client().download_file(S3_BUCKET, f'{S3_PREFIX}/{s3_key}',
                                   str(local_path))
        return True
    except Exception as e:
        print(f'  [S3] downloadå¤±æ•— {s3_key}: {e}')
        return False


def s3_list_keys(prefix: str = '') -> list:
    """S3_PREFIX/prefix ä»¥ä¸‹ã®ã‚­ãƒ¼ä¸€è¦§ã‚’è¿”ã™"""
    try:
        full_prefix = f'{S3_PREFIX}/{prefix}' if prefix else S3_PREFIX + '/'
        paginator = _s3_client().get_paginator('list_objects_v2')
        keys = []
        for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=full_prefix):
            for obj in page.get('Contents', []):
                keys.append(obj['Key'])
        return keys
    except Exception as e:
        print(f'  [S3] listå¤±æ•—: {e}')
        return []

TOP_N              = 100
RANDOM_PHASE_LIMIT = 200    # ã“ã®ä»¶æ•°ã¾ã§ã¯ç´”ãƒ©ãƒ³ãƒ€ãƒ ã€ä»¥é™ã¯ GA ä¸»ä½“
GA_RATIO           = 0.75   # GA ã®å‰²åˆ (æ®‹ã‚Šã¯ãƒ©ãƒ³ãƒ€ãƒ )
GA_PARENT_POOL     = 20     # è¦ªå€™è£œã‚’ä¸Šä½ä½•ä»¶ã‹ã‚‰é¸ã¶ã‹
H100_MODE     = os.environ.get('H100_MODE', '0') == '1'
MAX_PARALLEL  = int(os.environ.get('MAX_PARALLEL', '3' if H100_MODE else '1'))
VRAM_PER_TRIAL= float(os.environ.get('VRAM_PER_TRIAL', '10'))   # GB

# â”€â”€ ãƒ•ãƒªãƒ¼ã‚ºæ¤œçŸ¥: GPUç„¡ä½¿ç”¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ãƒ•ã‚§ãƒ¼ã‚ºã« DATA_PREP_BUDGET ç§’ã®çŒ¶äºˆã‚’ä¸ãˆã€
# ãã‚Œä»¥é™ã‚‚ GPU ã‚’ä½¿ã£ã¦ã„ãªã‘ã‚Œã°å¼·åˆ¶çµ‚äº†
DATA_PREP_BUDGET  = 600    # ç§’: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®æœ€å¤§è¨±å®¹æ™‚é–“ (10åˆ†)
NO_GPU_TIMEOUT    = 900    # ç§’: GPUä½¿ç”¨ãªã—ã§ã“ã‚Œä»¥ä¸Šâ†’å¼·åˆ¶çµ‚äº† (15åˆ†)
LAUNCH_INTERVAL   = 5      # ç§’: è©¦è¡ŒæŠ•å…¥é–“éš” (CUDAåˆæœŸåŒ–ã®é‡è¤‡ã‚’é˜²ã)

ARCHS = [
    'mlp', 'gru_attn', 'bigru', 'lstm_attn',
    'cnn', 'tcn', 'cnn_gru', 'transformer', 'resnet', 'inception',
]

HIDDEN_MAP_LOCAL = {
    'mlp':         [32, 64, 128, 256, 512],
    'gru_attn':    [64, 128, 256, 512],
    'bigru':       [64, 128, 256],
    'lstm_attn':   [64, 128, 256, 512],
    'cnn':         [64, 128, 256, 512],
    'tcn':         [64, 128, 256, 512],
    'cnn_gru':     [64, 128, 256],
    'transformer': [64, 128, 256],
    'resnet':      [64, 128, 256, 512],
    'inception':   [64, 128, 256],
}
HIDDEN_MAP_H100 = {
    'mlp':         [512, 1024, 2048],
    'gru_attn':    [256, 512, 1024],
    'bigru':       [256, 512, 1024],
    'lstm_attn':   [256, 512, 1024],
    'cnn':         [256, 512, 1024],
    'tcn':         [256, 512, 1024],
    'cnn_gru':     [256, 512, 1024],
    'transformer': [256, 512, 1024],
    'resnet':      [256, 512, 1024, 2048],
    'inception':   [256, 512, 1024],
}
HIDDEN_MAP     = HIDDEN_MAP_H100  if H100_MODE else HIDDEN_MAP_LOCAL
# ä¸¦åˆ—3æœ¬ Ã— æœ€å¤§ãƒãƒƒãƒã‚’è€ƒæ…®: H100 80GB / 3 â‰ˆ 26GB/è©¦è¡Œ
# å¤§ãƒ¢ãƒ‡ãƒ«(hâ‰¥1024)ã§ã¯å°ãƒãƒƒãƒã€å°ãƒ¢ãƒ‡ãƒ«ã§ã¯å¤§ãƒãƒƒãƒ
# H100: å°ãƒãƒƒãƒã§1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã‚’å¢—ã‚„ã—GPUç¨¼åƒç‡ã‚’ä¸Šã’ã‚‹
# ãƒ‡ãƒ¼ã‚¿13Kä»¶ / 512 = 25ãƒãƒƒãƒ/ep â†’ GPUç¨¼åƒç‡ ~60-80%
BATCH_CHOICES  = [256, 512, 1024, 2048] if H100_MODE else [256, 512, 1024, 2048]
SEQ_CHOICES    = [10, 15, 20, 30, 40, 50]  if H100_MODE else [5, 8, 10, 15, 20]
EPOCH_COUNT    = 2000 if H100_MODE else 800
TRIAL_TIMEOUT  = 5400 if H100_MODE else 600   # 90åˆ† (torch.compileè€ƒæ…®)


# â”€â”€ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sample_params(rng: random.Random) -> dict:
    arch    = rng.choice(ARCHS)
    hidden  = rng.choice(HIDDEN_MAP[arch])
    layers  = rng.choice([1, 2, 3] if arch not in ('mlp', 'gru_attn') else [1, 2])
    dropout = round(rng.uniform(0.3, 0.6), 1)
    lr      = rng.choice([1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 2e-3]
                         if H100_MODE else [1e-4, 3e-4, 5e-4, 8e-4, 1e-3])
    # å¤§ãƒ¢ãƒ‡ãƒ«ã§ã¯å°ãƒãƒƒãƒå¼·åˆ¶ (CUDA OOMé˜²æ­¢: 3ä¸¦åˆ— Ã— 26GB/trial)
    if H100_MODE and hidden >= 1024:
        batch = rng.choice([256, 512, 1024])
    else:
        batch = rng.choice(BATCH_CHOICES)
    tp      = round(rng.uniform(1.5, 3.5), 1)
    sl      = round(rng.uniform(0.5, 1.5), 1)
    fwd     = rng.choice([10, 15, 20, 25, 30])
    thr     = round(rng.uniform(0.33, 0.50), 2)
    seq_len = rng.choice(SEQ_CHOICES)
    sched   = rng.choice(['onecycle', 'cosine', 'cosine'])
    wd      = rng.choice([1e-3, 1e-2, 5e-2, 1e-1])
    tm      = rng.choice([0, 0, 0, 12, 18, 12])
    if rng.random() < 0.25:
        n_feat   = rng.randint(2, 70)
        feat_set = -1
    else:
        feat_set = rng.randint(0, len(FEATURE_SETS) - 1)
        n_feat   = len(FEATURE_SETS[feat_set])
    seed = rng.randint(0, 9999)
    return dict(
        arch=arch, hidden=hidden, layers=layers, dropout=dropout,
        lr=lr, batch=batch, tp=tp, sl=sl, forward=fwd,
        threshold=thr, seq_len=seq_len, scheduler=sched,
        wd=wd, train_months=tm, feat_set=feat_set, n_features=n_feat,
        seed=seed, timeframe='H1', epochs=EPOCH_COUNT,
        label_type='triple_barrier',
    )


# â”€â”€ éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _apply_one_mutation(p: dict, key: str, rng: random.Random) -> None:
    """key ã«å¯¾å¿œã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’1ã¤å¤‰ç•°ã•ã›ã‚‹ (in-place)"""
    if key == 'arch':
        p['arch']    = rng.choice(ARCHS)
        p['hidden']  = rng.choice(HIDDEN_MAP[p['arch']])
    elif key == 'hidden':
        p['hidden']  = rng.choice(HIDDEN_MAP[p['arch']])
    elif key == 'layers':
        p['layers']  = rng.choice([1, 2, 3] if p['arch'] not in ('mlp','gru_attn') else [1,2])
    elif key == 'dropout':
        p['dropout'] = round(rng.uniform(0.2, 0.7), 1)
    elif key == 'lr':
        p['lr']      = rng.choice([1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 2e-3]
                                   if H100_MODE else [1e-4, 3e-4, 5e-4, 8e-4, 1e-3])
    elif key == 'batch':
        p['batch']   = (rng.choice([2048, 4096, 8192]) if H100_MODE and p['hidden'] >= 1024
                        else rng.choice(BATCH_CHOICES))
    elif key == 'tp':
        p['tp']      = round(rng.uniform(1.2, 4.0), 1)
    elif key == 'sl':
        p['sl']      = round(rng.uniform(0.5, 2.0), 1)
    elif key == 'forward':
        p['forward'] = rng.choice([10, 15, 20, 25, 30, 40])
    elif key == 'threshold':
        p['threshold'] = round(rng.uniform(0.33, 0.55), 2)
    elif key == 'seq_len':
        p['seq_len'] = rng.choice(SEQ_CHOICES)
    elif key == 'scheduler':
        p['sched']   = rng.choice(['onecycle', 'cosine'])
    elif key == 'wd':
        p['wd']      = rng.choice([1e-4, 1e-3, 1e-2, 5e-2, 1e-1])
    elif key == 'train_months':
        p['train_months'] = rng.choice([0, 0, 12, 18, 24, 12])
    elif key == 'feat_set':
        # ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚»ãƒƒãƒˆã‚’å¤‰ãˆã‚‹ (æ¢ç´¢å¤šæ§˜æ€§å‘ä¸Š)
        p['feat_set'] = rng.randint(0, 99)


def _mutate(params: dict, rng: random.Random) -> dict:
    """è¤‡æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ç•°ã•ã›ã‚‹ (1ã€œ3å€‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ)"""
    p = dict(params)
    mut_keys = [
        'arch', 'hidden', 'layers', 'dropout', 'lr', 'batch',
        'tp', 'sl', 'forward', 'threshold', 'seq_len',
        'scheduler', 'wd', 'train_months', 'feat_set',
    ]
    # å¤‰ç•°æ•°: å¤šæ§˜æ€§ã®ãŸã‚1ã€œ3å€‹
    n_mut = rng.choices([1, 2, 3], weights=[0.5, 0.35, 0.15])[0]
    chosen = rng.sample(mut_keys, n_mut)
    for key in chosen:
        _apply_one_mutation(p, key, rng)
    # arch/hidden ã®æ•´åˆæ€§ã‚’ä¿è¨¼
    if p['hidden'] not in HIDDEN_MAP.get(p['arch'], [p['hidden']]):
        p['hidden'] = rng.choice(HIDDEN_MAP[p['arch']])
    p['seed'] = rng.randint(0, 9999)
    return p


def _crossover(p1: dict, p2: dict, rng: random.Random) -> dict:
    """2 ã¤ã®è¦ªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ 1 ç‚¹äº¤å‰ã§æ··åˆ"""
    keys = [
        'arch', 'hidden', 'layers', 'dropout', 'lr', 'batch',
        'tp', 'sl', 'forward', 'threshold', 'seq_len',
        'scheduler', 'wd', 'train_months', 'feat_set', 'n_features',
    ]
    child = dict(p1)
    for k in keys:
        if rng.random() < 0.5 and k in p2:
            child[k] = p2[k]
    # arch ã¨ hidden ã®çµ„ã¿åˆã‚ã›ãŒå´©ã‚Œã¦ã„ãŸã‚‰ä¿®æ­£
    if child['hidden'] not in HIDDEN_MAP.get(child['arch'], [child['hidden']]):
        child['hidden'] = rng.choice(HIDDEN_MAP[child['arch']])
    child['seed'] = rng.randint(0, 9999)
    child['epochs'] = EPOCH_COUNT
    child['timeframe'] = 'H1'
    child['label_type'] = 'triple_barrier'
    return child


def _tournament_select(pool: list, rng: random.Random, k: int = 4) -> dict:
    """ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé¸æŠ: pool ã‹ã‚‰ k ä»¶ã‚’å¼•ã„ã¦ PF æœ€å¤§ã‚’è¿”ã™"""
    candidates = rng.sample(pool, min(k, len(pool)))
    return max(candidates, key=lambda r: r['pf'])


def ga_sample(results: list, rng: random.Random) -> dict:
    """éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹"""
    valid = [r for r in results if r.get('pf', 0) > 0 and r.get('trades', 0) >= 200]
    if len(valid) < 2:
        return sample_params(rng)   # å€™è£œä¸è¶³ãªã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    # â”€â”€ è¦ªãƒ—ãƒ¼ãƒ«: ä¸Šä½ GA_PARENT_POOL ä»¶ (å¤šæ§˜æ€§ã®ãŸã‚ archãƒ»feat_set ãŒè¢«ã‚‰ãªã„ã‚ˆã†èª¿æ•´) â”€â”€
    sorted_valid = sorted(valid, key=lambda x: -x['pf'])
    pool = []
    seen_arch_feat: set = set()
    for r in sorted_valid:
        key = (r.get('arch', '?'), r.get('feat_set', -1))
        if key not in seen_arch_feat or len(pool) < GA_PARENT_POOL // 2:
            pool.append(r)
            seen_arch_feat.add(key)
        if len(pool) >= GA_PARENT_POOL:
            break

    r_val = rng.random()
    if r_val < 0.5:
        # äº¤å‰: è¦ª 2 ä½“ã‚’é¸ã‚“ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ··åˆ
        p1 = _tournament_select(pool, rng)
        p2 = _tournament_select(pool, rng)
        child = _crossover(p1, p2, rng)
    elif r_val < 0.85:
        # çªç„¶å¤‰ç•°: è¦ª 1 ä½“ã‹ã‚‰è¤‡æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã‚‹
        p1    = _tournament_select(pool, rng)
        child = _mutate(p1, rng)
    else:
        # 15%: ä¸Šä½ã‹ã‚‰è¦ªã‚’é¸ã‚“ã§ãƒ©ãƒ³ãƒ€ãƒ å¤§å¤‰ç•° (exploration)
        p1 = pool[0]  # best parent
        child = _mutate(p1, rng)
        # ã•ã‚‰ã«è¿½åŠ ã§ 1ã€œ2 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å†å¤‰ç•°
        extra = rng.sample(['arch', 'tp', 'sl', 'threshold', 'feat_set', 'forward'], 2)
        for k in extra:
            _apply_one_mutation(child, k, rng)
        if child['hidden'] not in HIDDEN_MAP.get(child['arch'], [child['hidden']]):
            child['hidden'] = rng.choice(HIDDEN_MAP[child['arch']])

    return child


def next_params(results: list, rng: random.Random) -> tuple[dict, str]:
    """å®Œäº†ä»¶æ•°ã«å¿œã˜ã¦ GA / ãƒ©ãƒ³ãƒ€ãƒ ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æˆ¦ç•¥åã‚’è¿”ã™"""
    n = len(results)
    if n < RANDOM_PHASE_LIMIT:
        return sample_params(rng), 'random'
    if rng.random() < GA_RATIO:
        return ga_sample(results, rng), 'GA'
    return sample_params(rng), 'random'


# â”€â”€ GPU æƒ…å ±å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gpu_info() -> dict:
    try:
        # nvidia-ml-py (pynvml ã®å¾Œç¶™ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸)
        from pynvml import (nvmlInit, nvmlDeviceGetHandleByIndex,
                            nvmlDeviceGetMemoryInfo, nvmlDeviceGetUtilizationRates)
        nvmlInit()
        h = nvmlDeviceGetHandleByIndex(0)
        m = nvmlDeviceGetMemoryInfo(h)
        u = nvmlDeviceGetUtilizationRates(h)
        return {
            'free_gb':  m.free  / 1e9,
            'total_gb': m.total / 1e9,
            'used_gb':  m.used  / 1e9,
            'gpu_pct':  u.gpu,
            'mem_pct':  round(m.used / m.total * 100),
        }
    except Exception:
        return {'free_gb': 999, 'total_gb': 80, 'used_gb': 0, 'gpu_pct': 0, 'mem_pct': 0}


def get_gpu_compute_pids() -> set:
    """nvidia-smi ã§ç¾åœ¨ GPU è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ PID ã‚»ãƒƒãƒˆã‚’è¿”ã™"""
    try:
        r = subprocess.run(
            ['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader'],
            capture_output=True, text=True, timeout=10
        )
        pids = set()
        for line in r.stdout.strip().split('\n'):
            line = line.strip()
            if line.isdigit():
                pids.add(int(line))
        return pids
    except Exception:
        return set()


def get_max_parallel(n_running: int) -> int:
    """VRAM/GPU ä½¿ç”¨ç‡ã‹ã‚‰å‹•çš„ã«æœ€å¤§ä¸¦åˆ—æ•°ã‚’è¿”ã™"""
    if not H100_MODE:
        return MAX_PARALLEL
    gi = _gpu_info()
    # VRAM ç©ºãã‹ã‚‰æ ã‚’è¨ˆç®—
    vram_slots = max(1, int(gi['free_gb'] / VRAM_PER_TRIAL))
    # GPU ãŒé«˜è² è·ãªã‚‰ç¶­æŒ
    if gi['gpu_pct'] > 92 and n_running > 0:
        return n_running
    # VRAMä¸è¶³ã§ã‚‚æœ€ä½1ä¸¦åˆ—ã¯ä¿è¨¼ (ãƒ•ãƒªãƒ¼ã‚ºé˜²æ­¢)
    return max(1, min(MAX_PARALLEL, vram_slots))


# â”€â”€ TOP_N ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_trial_model(trial_no: int) -> None:
    """ç¾åœ¨ã® ONNX ã¨ norm_params ã‚’ top_cache ã«ä¿å­˜"""
    trial_dir = TRIALS_DIR / f'trial_{trial_no:06d}'
    dest = TOP_CACHE_DIR / f'trial_{trial_no:06d}'
    dest.mkdir(parents=True, exist_ok=True)
    for fname in ['fx_model.onnx', 'norm_params.json', 'report.html']:
        src = trial_dir / fname
        if src.exists():
            shutil.copy2(src, dest / fname)


def rebuild_top_n(results: list) -> None:
    """all_results ã‹ã‚‰ TOP_N ã‚’è¨ˆç®—ã—ã¦ top100/rank_XXX/ ã‚’å†æ§‹ç¯‰"""
    valid = [r for r in results
             if r.get('pf', 0) > 0 and r.get('trades', 0) >= 200]
    top_n = sorted(valid, key=lambda x: -x['pf'])[:TOP_N]
    TOP_DIR.mkdir(parents=True, exist_ok=True)
    for rank, r in enumerate(top_n, 1):
        tno = r.get('trial', 0)
        src = TOP_CACHE_DIR / f'trial_{tno:06d}'
        dst = TOP_DIR / f'rank_{rank:03d}'
        if src.exists():
            if dst.exists():
                shutil.rmtree(dst)
            shutil.copytree(src, dst)
            (dst / 'result.json').write_text(
                json.dumps(r, indent=2, ensure_ascii=False), encoding='utf-8')


# â”€â”€ é›†ç´„ progress.json â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_progress(running: dict, results: list, best_pf: float, start: float) -> None:
    running_info = []
    gi = _gpu_info()
    for tno, info in list(running.items()):
        tp_file = info['trial_dir'] / 'trial_progress.json'
        tp = {}
        if tp_file.exists():
            try:
                tp = json.loads(tp_file.read_text(encoding='utf-8'))
            except Exception:
                pass
        running_info.append({
            'trial':       tno,
            'arch':        info['params'].get('arch', '?'),
            'hidden':      info['params'].get('hidden', '?'),
            'epoch':       tp.get('epoch', 0),
            'total_epochs':tp.get('total_epochs', EPOCH_COUNT),
            'train_loss':  tp.get('train_loss', 0.0),
            'val_loss':    tp.get('val_loss', 0.0),
            'accuracy':    tp.get('accuracy', 0.0),
            'phase':       tp.get('phase', 'running'),
            'strategy':    info.get('strategy', 'random'),
            'elapsed_sec': round(time.time() - info['start_time'], 0),
        })

    # æœ€è¿‘ã® trial çµæœ (epoch_log ç”¨ã«æœ€æ–° running trial ã® log ã‚’ä½¿ã†)
    epoch_log = []
    if running_info:
        latest = max(running_info, key=lambda x: x['trial'])
        tp_file = (TRIALS_DIR / f"trial_{latest['trial']:06d}" / 'trial_progress.json')
        if tp_file.exists():
            try:
                epoch_log = json.loads(tp_file.read_text(encoding='utf-8')).get('epoch_log', [])
            except Exception:
                pass

    n_done    = len(results)
    search_phase = ('random' if n_done < RANDOM_PHASE_LIMIT
                    else f'GA {int(GA_RATIO*100)}% + random {int((1-GA_RATIO)*100)}%')
    progress = {
        'phase':           'training' if running else 'waiting',
        'search_phase':    search_phase,
        'completed_count': n_done,
        'random_phase_limit': RANDOM_PHASE_LIMIT,
        'running_count':   len(running),
        'running_trials':  running_info,
        'best_pf':         best_pf,
        'target_pf':       0,
        'epoch_log':       epoch_log,
        'trial_results':   results[-200:],
        'start_time':      start,
        'elapsed_sec':     time.time() - start,
        'gpu_pct':         gi['gpu_pct'],
        'vram_used_gb':    round(gi['used_gb'], 1),
        'vram_total_gb':   round(gi['total_gb'], 1),
        'message': (f"å®Ÿè¡Œä¸­: {len(running)}ä¸¦åˆ—  å®Œäº†: {n_done}ä»¶  "
                    f"ãƒ™ã‚¹ãƒˆ PF: {best_pf:.4f}  [{search_phase}]  "
                    f"GPU: {gi['gpu_pct']}%  VRAM: {gi['used_gb']:.1f}/{gi['total_gb']:.0f}GB"),
    }
    try:
        tmp = PROGRESS_JSON.with_suffix('.tmp')
        tmp.write_text(json.dumps(progress, ensure_ascii=False, indent=2), encoding='utf-8')
        tmp.replace(PROGRESS_JSON)
    except Exception as e:
        print(f"  [WARN] progress.json æ›¸ãè¾¼ã¿å¤±æ•—: {e}")


# â”€â”€ ä¸¦åˆ—ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ParallelTrainer:
    def __init__(self):
        self.running: dict = {}   # trial_no -> {proc, params, start_time, trial_dir, log_fh}
        self.lock = threading.Lock()

    def launch(self, trial_no: int, params: dict, best_pf: float, start_time: float,
               strategy: str = 'random'):
        trial_dir = TRIALS_DIR / f'trial_{trial_no:06d}'
        trial_dir.mkdir(parents=True, exist_ok=True)

        cmd = [PY, str(TRAIN_PY),
               '--trial',        str(trial_no),
               '--total_trials', '99999',
               '--best_pf',      str(best_pf),
               '--start_time',   str(start_time),
               '--out_dir',      str(trial_dir),
               ]
        for k, v in params.items():
            cmd += [f'--{k}', str(v)]

        log_fh = open(trial_dir / 'train.log', 'w', encoding='utf-8', buffering=1)
        proc   = subprocess.Popen(cmd, stdout=log_fh, stderr=subprocess.STDOUT)

        with self.lock:
            self.running[trial_no] = {
                'proc':       proc,
                'params':     params,
                'start_time': time.time(),
                'trial_dir':  trial_dir,
                'log_fh':     log_fh,
                'strategy':   strategy,
            }
        feat_info = (f"set#{params['feat_set']}"
                     if params.get('feat_set', -1) >= 0 else f"rand{params['n_features']}")
        tag = 'ğŸ§¬GA' if strategy == 'GA' else 'ğŸ²Rnd'
        print(f"  [LAUNCH] è©¦è¡Œ#{trial_no:4d} {tag}  {params['arch']:12s}  "
              f"h={params['hidden']:4d}  feat={feat_info}  PID={proc.pid}")

    def poll_completed(self) -> list:
        """å®Œäº†/ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸè©¦è¡Œã®ãƒªã‚¹ãƒˆã‚’è¿”ã— running ã‹ã‚‰å‰Šé™¤"""
        done = []
        gpu_pids = get_gpu_compute_pids()   # ç¾åœ¨GPUä½¿ç”¨ä¸­ã®PIDã‚»ãƒƒãƒˆ
        now = time.time()

        with self.lock:
            for tno in list(self.running.keys()):
                info    = self.running[tno]
                elapsed = now - info['start_time']
                proc    = info['proc']

                if proc.poll() is None:   # ã¾ã å®Ÿè¡Œä¸­
                    pid = proc.pid

                    # â”€â”€ GPUä½¿ç”¨ä¸­PIDã®è¿½è·¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if pid in gpu_pids:
                        info['last_gpu_time'] = now   # GPUã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ™‚åˆ»ã‚’æ›´æ–°

                    last_gpu = info.get('last_gpu_time')
                    since_gpu = (now - last_gpu) if last_gpu else elapsed

                    # â”€â”€ GPUãƒãƒ¼ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¦ã‚©ãƒƒãƒãƒ‰ãƒƒã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # DATA_PREP_BUDGET ç§’ä»¥å†…ã¯ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­ã¨ã—ã¦è¨±å®¹
                    # ãã‚Œä»¥é™ã‚‚ GPU ã‚’ä½¿ã£ã¦ã„ãªã‘ã‚Œã°å¼·åˆ¶çµ‚äº†
                    if elapsed > DATA_PREP_BUDGET and since_gpu > NO_GPU_TIMEOUT:
                        print(f"  [NO-GPU] è©¦è¡Œ#{tno}  çµŒé{elapsed/60:.1f}åˆ†"
                              f"  GPUç„¡ä½¿ç”¨{since_gpu/60:.1f}åˆ† â†’ å¼·åˆ¶çµ‚äº†")
                        try:
                            proc.terminate()
                            proc.wait(timeout=10)
                        except Exception:
                            proc.kill()

                    # â”€â”€ å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    elif elapsed > TRIAL_TIMEOUT:
                        print(f"  [TIMEOUT] è©¦è¡Œ#{tno} ({elapsed/60:.0f}åˆ†è¶…) â†’ å¼·åˆ¶çµ‚äº†")
                        try:
                            proc.terminate()
                            proc.wait(timeout=10)
                        except Exception:
                            proc.kill()

                if proc.poll() is not None:
                    info['log_fh'].close()
                    done.append((tno, info))
                    del self.running[tno]
        return done

    def terminate_all(self):
        with self.lock:
            for info in self.running.values():
                try:
                    info['proc'].terminate()
                except Exception:
                    pass

    def __len__(self):
        return len(self.running)


# â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»å¾©å…ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_checkpoint(results: list, best_pf: float) -> None:
    """all_results + best model + top100 ã‚’ãƒ­ãƒ¼ã‚«ãƒ« & S3 ã«ä¿å­˜"""
    try:
        CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
        # all_results.json
        tmp = CHECKPOINT_DIR / 'all_results.json.tmp'
        tmp.write_text(json.dumps(results, indent=2, ensure_ascii=False), encoding='utf-8')
        tmp.replace(CHECKPOINT_DIR / 'all_results.json')
        # best model ãƒ•ã‚¡ã‚¤ãƒ«
        for src, name in [(BEST_ONNX, 'fx_model_best.onnx'),
                          (BEST_NORM, 'norm_params_best.json'),
                          (BEST_JSON, 'best_result.json')]:
            if src.exists():
                shutil.copy2(src, CHECKPOINT_DIR / name)
        # top100 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        top_dst = CHECKPOINT_DIR / 'top100'
        if TOP_DIR.exists():
            if top_dst.exists():
                shutil.rmtree(top_dst)
            shutil.copytree(TOP_DIR, top_dst)
        # ãƒ¡ã‚¿æƒ…å ±
        meta = {'saved_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'completed': len(results), 'best_pf': best_pf,
                's3': S3_ENABLED}
        (CHECKPOINT_DIR / 'meta.json').write_text(
            json.dumps(meta, ensure_ascii=False), encoding='utf-8')
        print(f'  [CKPT] ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å®Œäº† ({len(results)}ä»¶ / bestPF={best_pf:.4f})')

        # S3 ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if S3_ENABLED:
            upload_files = ['all_results.json', 'meta.json',
                            'fx_model_best.onnx', 'norm_params_best.json', 'best_result.json']
            ok = 0
            for name in upload_files:
                p = CHECKPOINT_DIR / name
                if p.exists() and s3_upload(p, name):
                    ok += 1
            # top100 ã‚’ S3 ã«åŒæœŸ
            top100_ok = 0
            if top_dst.exists():
                for f in top_dst.rglob('*'):
                    if f.is_file():
                        rel = f.relative_to(CHECKPOINT_DIR)
                        if s3_upload(f, str(rel).replace('\\', '/')):
                            top100_ok += 1
            print(f'  [S3]  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº† ({ok}/{len(upload_files)}ä»¶ + top100:{top100_ok}ä»¶) '
                  f'â†’ s3://{S3_BUCKET}/{S3_PREFIX}/')
        else:
            print(f'  [CKPT] S3æœªè¨­å®š â†’ ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ä¿å­˜ ({CHECKPOINT_DIR})')
    except Exception as e:
        print(f'  [CKPT] ä¿å­˜å¤±æ•—: {e}')


def restore_checkpoint() -> bool:
    """S3 â†’ ãƒ­ãƒ¼ã‚«ãƒ« â†’ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ã®é †ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å¾©å…ƒ"""
    # S3 ã‹ã‚‰å…ˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
    if S3_ENABLED:
        print(f'  [S3]  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèªä¸­ s3://{S3_BUCKET}/{S3_PREFIX}/ ...')
        dl_files = ['all_results.json', 'meta.json',
                    'fx_model_best.onnx', 'norm_params_best.json', 'best_result.json']
        downloaded = 0
        for name in dl_files:
            if s3_download(name, CHECKPOINT_DIR / name):
                downloaded += 1
        # top100 ã¯ result.json ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ONNXã¯å¤§ãã„ã®ã§èµ·å‹•æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—)
        top100_json_count = 0
        for key in s3_list_keys('top100'):
            if not key.endswith('result.json'):
                continue   # ONNX / norm_params / report.html ã¯ã‚¹ã‚­ãƒƒãƒ—
            rel  = key[len(S3_PREFIX)+1:]
            dest = CHECKPOINT_DIR / rel
            dest.parent.mkdir(parents=True, exist_ok=True)
            if s3_download(key[len(S3_PREFIX)+1:], dest):
                top100_json_count += 1
        if top100_json_count:
            print(f'  [S3]  top100 result.json {top100_json_count}ä»¶ å–å¾— (ONNX ã¯ã‚¹ã‚­ãƒƒãƒ—)')
        if downloaded == 0:
            print('  [S3]  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãªã—')

    # ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰å¾©å…ƒ
    meta_path = CHECKPOINT_DIR / 'meta.json'
    ar_path   = CHECKPOINT_DIR / 'all_results.json'
    if not ar_path.exists():
        return False
    try:
        meta = json.loads(meta_path.read_text(encoding='utf-8')) if meta_path.exists() else {}
        print(f'  [CKPT] ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹: {meta.get("saved_at","?")}  '
              f'{meta.get("completed","?")}ä»¶  bestPF={meta.get("best_pf","?")}')
        shutil.copy2(ar_path, ALL_RESULTS)
        for name, dst in [('fx_model_best.onnx',   BEST_ONNX),
                          ('norm_params_best.json', BEST_NORM),
                          ('best_result.json',       BEST_JSON)]:
            src = CHECKPOINT_DIR / name
            if src.exists():
                shutil.copy2(src, dst)
        top_src = CHECKPOINT_DIR / 'top100'
        if top_src.exists():
            if TOP_DIR.exists():
                shutil.rmtree(TOP_DIR)
            shutil.copytree(top_src, TOP_DIR)
        print('  [CKPT] å¾©å…ƒå®Œäº† â†’ å‰å›ã®ç¶šãã‹ã‚‰å†é–‹ã—ã¾ã™')
        return True
    except Exception as e:
        print(f'  [CKPT] å¾©å…ƒå¤±æ•—: {e}')
        return False


# â”€â”€ ãƒ¡ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _precache_data() -> bool:
    """ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ä½œæˆã—ã¦å…¨è©¦è¡ŒãŒå³åº§ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹"""
    import pickle
    DATA_PATH = Path(os.environ.get('DATA_PATH', '/workspace/data/USDJPY_M1.csv'))
    cache_path = TRIALS_DIR.parent / 'df_cache_H1.pkl'
    if cache_path.exists():
        print(f"  [PRE-CACHE] ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ—¢å­˜: {cache_path}")
        return True
    if not DATA_PATH.exists():
        print(f"  [PRE-CACHE] ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {DATA_PATH}")
        return False
    print(f"  [PRE-CACHE] ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ä½œæˆä¸­... (åˆå›ã®ã¿æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)")
    try:
        import sys as _sys
        _sys.path.insert(0, str(TRAIN_PY.parent))
        from features import load_data, add_indicators
        import numpy as np
        from datetime import timedelta
        t0 = time.time()
        df = load_data(str(DATA_PATH), timeframe='H1')
        df = add_indicators(df)
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(inplace=True)
        test_start = df.index[-1] - timedelta(days=365)
        df_tr = df[df.index < test_start].copy()
        df_te = df[df.index >= test_start].copy()
        tmp = cache_path.with_suffix('.tmp')
        with open(tmp, 'wb') as f:
            pickle.dump((df_tr, df_te), f)
        tmp.replace(cache_path)
        print(f"  [PRE-CACHE] å®Œäº† {time.time()-t0:.1f}ç§’  "
              f"è¨“ç·´:{len(df_tr):,}è¡Œ  ãƒ†ã‚¹ãƒˆ:{len(df_te):,}è¡Œ  â†’ {cache_path}")
        return True
    except Exception as e:
        print(f"  [PRE-CACHE] å¤±æ•— (è¨“ç·´ã¯ç¶šè¡Œ): {e}")
        return False


def main():
    # SIGTERM (ã‚³ãƒ³ãƒ†ãƒŠåœæ­¢æ™‚) ã‚’å—ã‘å–ã£ãŸã‚‰ stop.flag ã‚’ç½®ã„ã¦graceful shutdown
    def _sigterm_handler(signum, frame):
        print('\n[SIGNAL] SIGTERM å—ä¿¡ â†’ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã—ã¦åœæ­¢ã—ã¾ã™...')
        STOP_FLAG.touch()
    signal.signal(signal.SIGTERM, _sigterm_handler)
    signal.signal(signal.SIGINT,  _sigterm_handler)

    TRIALS_DIR.mkdir(parents=True, exist_ok=True)
    TOP_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    TOP_DIR.mkdir(parents=True, exist_ok=True)

    mode_str = f'H100 80GB  ä¸¦åˆ—={MAX_PARALLEL}  VRAM/è©¦è¡Œ={VRAM_PER_TRIAL}GB' \
               if H100_MODE else 'GTX 1080 Ti  ã‚·ãƒ³ã‚°ãƒ«'
    print('=' * 60)
    print(f'FX AI EA v8 - ä¸¦åˆ—ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ [{mode_str}]')
    print(f'  TOP {TOP_N} ä¿å­˜  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ {TRIAL_TIMEOUT//60}åˆ†  stop.flag: {STOP_FLAG}')
    print(f'  GPUç„¡ä½¿ç”¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {NO_GPU_TIMEOUT//60}åˆ†  ãƒ‡ãƒ¼ã‚¿æº–å‚™çŒ¶äºˆ: {DATA_PREP_BUDGET//60}åˆ†')
    print('=' * 60)

    # â”€â”€ S3 æ¥ç¶šç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f'  S3_ENABLED : {S3_ENABLED}')
    print(f'  S3_ENDPOINT: {S3_ENDPOINT or "(æœªè¨­å®š)"}')
    print(f'  S3_BUCKET  : {S3_BUCKET}  PREFIX: {S3_PREFIX}')
    if S3_ENABLED:
        try:
            cl = _s3_client()
            cl.put_object(Bucket=S3_BUCKET, Key=f'{S3_PREFIX}/.ping', Body=b'ok')
            cl.delete_object(Bucket=S3_BUCKET, Key=f'{S3_PREFIX}/.ping')
            print('  [S3] æ¥ç¶šãƒ†ã‚¹ãƒˆ OK âœ…')
        except Exception as e:
            print(f'  [S3] æ¥ç¶šãƒ†ã‚¹ãƒˆ å¤±æ•— âŒ: {e}')
    else:
        print('  [S3] ç„¡åŠ¹ (S3_ENDPOINT/S3_ACCESS_KEY/S3_SECRET_KEY ã‚’ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„)')

    # â”€â”€ èµ·å‹•æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ä½œæˆ (å…¨è©¦è¡ŒãŒå³åº§ã«å­¦ç¿’é–‹å§‹ã§ãã‚‹) â”€â”€
    _precache_data()

    if STOP_FLAG.exists():
        STOP_FLAG.unlink()

    rng      = random.Random()
    trainer  = ParallelTrainer()
    results  = []
    best_pf  = 0.0
    trial_no = 1
    start    = time.time()

    # â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ (ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚¦ãƒ³ãƒˆæ™‚ã¯è‡ªå‹•ç¶™ç¶š) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not ALL_RESULTS.exists():
        restore_checkpoint()

    # æ—¢å­˜çµæœã‚’å¼•ãç¶™ã
    if ALL_RESULTS.exists():
        try:
            raw = json.loads(ALL_RESULTS.read_text(encoding='utf-8'))
            # â”€â”€ é‡è¤‡æ’é™¤: åŒã˜ trial ç•ªå·ã¯æœ€åˆã®1ä»¶ã®ã¿æ®‹ã™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            seen: set = set()
            results = []
            for r in raw:
                tno_r = r.get('trial', -1)
                if tno_r not in seen:
                    seen.add(tno_r)
                    results.append(r)
            if len(raw) != len(results):
                print(f"  [DEDUP] é‡è¤‡é™¤å»: {len(raw)} â†’ {len(results)} ä»¶")
                # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãä¿å­˜
                tmp = ALL_RESULTS.with_suffix('.tmp')
                tmp.write_text(json.dumps(results, indent=2, ensure_ascii=False),
                               encoding='utf-8')
                tmp.replace(ALL_RESULTS)
            trial_no = max((r.get('trial', 0) for r in results), default=0) + 1
            valid    = [r for r in results if r.get('pf', 0) > 0]
            if valid:
                best_r  = max(valid, key=lambda r: r['pf'])
                best_pf = best_r['pf']
                print(f"  å‰å›æœ€è‰¯PF={best_pf:.4f}  å®Œäº†{len(results)}ä»¶  æ¬¡è©¦è¡Œ#{trial_no}")
        except Exception:
            pass

    last_checkpoint        = time.time()
    completed_since_ckpt   = 0   # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾Œã®å®Œäº†ä»¶æ•°ã‚«ã‚¦ãƒ³ã‚¿

    write_progress(trainer.running, results, best_pf, start)

    # â”€â”€ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    while True:
        # stop.flag ãƒã‚§ãƒƒã‚¯
        if STOP_FLAG.exists():
            print(f"\n[STOP] stop.flag æ¤œå‡º â†’ å®Ÿè¡Œä¸­ã®è©¦è¡Œã‚’å¾…æ©Ÿã—ã¦çµ‚äº†")
            trainer.terminate_all()
            break

        # â”€â”€ å®Œäº†ã—ãŸè©¦è¡Œã‚’å›å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        newly_done = trainer.poll_completed()
        completed_since_ckpt += len(newly_done)
        for tno, info in newly_done:
            result_path = info['trial_dir'] / 'last_result.json'
            r = {}
            if result_path.exists():
                try:
                    r = json.loads(result_path.read_text(encoding='utf-8'))
                except Exception:
                    pass

            pf     = float(r.get('pf', 0.0))
            trades = int(r.get('trades', 0))
            sr     = float(r.get('sr', 0.0))
            max_dd = float(r.get('max_dd', 0.0))
            elapsed= round(time.time() - info['start_time'], 0)

            record = {
                'trial':     tno,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'strategy':  info.get('strategy', 'random'),
                'pf':        pf,
                'trades':    trades,
                'win_rate':  r.get('win_rate',      0.0),
                'net_pnl':   r.get('net_pnl',       0.0),
                'gross_profit': r.get('gross_profit', 0.0),
                'gross_loss':   r.get('gross_loss',   0.0),
                'sr':        sr,
                'max_dd':    max_dd,
                'elapsed_sec': elapsed,
                **{k: v for k, v in info['params'].items()},
            }
            # é‡è¤‡é˜²æ­¢: åŒã˜ trial_no ãŒã™ã§ã«ã‚ã‚Œã°ä¸Šæ›¸ãã€ãªã‘ã‚Œã°è¿½åŠ 
            existing_idx = next((i for i, r in enumerate(results) if r['trial'] == tno), None)
            if existing_idx is not None:
                results[existing_idx] = record
            else:
                results.append(record)
            results.sort(key=lambda x: x['trial'])

            # all_results.json ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿
            try:
                tmp = ALL_RESULTS.with_suffix('.tmp')
                tmp.write_text(json.dumps(results, indent=2, ensure_ascii=False),
                               encoding='utf-8')
                tmp.replace(ALL_RESULTS)
            except Exception as e:
                print(f"  [WARN] çµæœä¿å­˜å¤±æ•—: {e}")

            # TOP_N ã«å…¥ã£ãŸã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦å†æ§‹ç¯‰
            if pf > 0 and trades >= 200:
                try:
                    save_trial_model(tno)
                    rebuild_top_n(results)
                except Exception as e:
                    print(f"  [WARN] TOP{TOP_N} æ›´æ–°å¤±æ•—: {e}")

            # ãƒ™ã‚¹ãƒˆæ›´æ–° (200å–å¼•ä»¥ä¸Šã®ã¿å¯¾è±¡)
            if pf > best_pf and trades >= 200:
                best_pf = pf
                for src, dst in [(info['trial_dir'] / 'fx_model.onnx',    BEST_ONNX),
                                  (info['trial_dir'] / 'norm_params.json', BEST_NORM)]:
                    if src.exists():
                        shutil.copy2(src, dst)
                BEST_JSON.write_text(
                    json.dumps({**info['params'], 'pf': best_pf,
                                'sr': sr, 'max_dd': max_dd, 'trial': tno},
                               indent=2, ensure_ascii=False), encoding='utf-8')
                print(f"  [BEST] è©¦è¡Œ#{tno}  PF={pf:.4f}  SR={sr:.3f}  MaxDD={max_dd:.4f}")
            else:
                print(f"  [DONE] è©¦è¡Œ#{tno:4d}  PF={pf:.4f}  SR={sr:.3f}  "
                      f"MaxDD={max_dd:.4f}  å–å¼•={trades}  "
                      f"{elapsed/60:.1f}åˆ†  (ãƒ™ã‚¹ãƒˆ={best_pf:.4f})")

        # â”€â”€ æ–°è¦è©¦è¡Œã‚’æŠ•å…¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        max_par = get_max_parallel(len(trainer))
        while len(trainer) < max_par:
            if STOP_FLAG.exists():
                break
            p, strategy = next_params(results, rng)
            trainer.launch(trial_no, p, best_pf, start, strategy)
            trial_no += 1
            time.sleep(LAUNCH_INTERVAL)   # é€£ç¶šèµ·å‹•ã®é–“éš” (CUDAåˆæœŸåŒ–ã®é‡è¤‡ã‚’é˜²ã)

        # â”€â”€ é€²æ— JSON æ›¸ãè¾¼ã¿ (5ç§’ã”ã¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        write_progress(trainer.running, results, best_pf, start)

        # â”€â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: 10è©¦è¡Œã”ã¨ or 10åˆ†ã”ã¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        should_ckpt = (completed_since_ckpt >= CHECKPOINT_EVERY_N or
                       time.time() - last_checkpoint >= CHECKPOINT_INTERVAL)
        if should_ckpt:
            save_checkpoint(results, best_pf)
            last_checkpoint      = time.time()
            completed_since_ckpt = 0

        time.sleep(5)

    # â”€â”€ çµ‚äº†å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    write_progress({}, results, best_pf, start)
    save_checkpoint(results, best_pf)   # åœæ­¢æ™‚ã«å¿…ãšãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    print(f"\nå®Œäº†  ç·è©¦è¡Œ: {len(results)}ä»¶  æœ€è‰¯PF: {best_pf:.4f}")
    if BEST_ONNX.exists():
        shutil.copy2(BEST_ONNX, OUT_DIR / 'fx_model.onnx')
    if BEST_NORM.exists():
        shutil.copy2(BEST_NORM, OUT_DIR / 'norm_params.json')

    # â”€â”€ MT5 Common\Files ã¸è‡ªå‹•ã‚³ãƒ”ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _appdata = Path(os.environ.get('APPDATA', ''))
    _common  = _appdata / 'MetaQuotes' / 'Terminal' / 'Common' / 'Files'
    if _common.exists():
        _copies = [
            (OUT_DIR / 'fx_model.onnx',    _common / 'fx_model.onnx'),
            (OUT_DIR / 'norm_params.json',  _common / 'norm_params.json'),
        ]
        for src, dst in _copies:
            if src.exists():
                shutil.copy2(src, dst)
                print(f"  â†’ Common\\Files\\ ã«ã‚³ãƒ”ãƒ¼: {src.name}")
    else:
        print(f"  [skip] Common\\Files æœªæ¤œå‡º: {_common}")


if __name__ == '__main__':
    main()
