version: "3.9"

services:
  fx-ea:
    build:
      context: ..
      dockerfile: DOK_EA/Dockerfile
    image: fx-ea:latest
    container_name: fx-ea-h100

    # H100 GPU を全台使用
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    ports:
      - "7860:7860"   # ダッシュボード
      - "2222:22"     # SSH

    environment:
      # H100 80GB モード (大型モデル・大バッチ)
      H100_MODE: "1"
      # 並列学習数 (VRAM 空き監視で動的調整)
      MAX_PARALLEL: "3"
      # 1試行あたりの最低 VRAM (GB)
      VRAM_PER_TRIAL: "10"
      # バックテスト設定 (MQL5 LotSize相当)
      BT_CAPITAL: "150000"
      BT_LEVERAGE: "1000"
      BT_RISK_PCT: "1.0"       # 残高の1%をリスクにロット決定 (変更可: 0.5〜2.0)
      # データ CSV の URL (起動時に自動ダウンロード)
      # DATA_URL: "https://your-storage/USDJPY_M1.csv"
      DATA_PATH: "/workspace/data/USDJPY_M1.csv"

    volumes:
      # CSV データ永続化
      - ./data:/workspace/data
      # 学習結果・TOP100モデル・ログ永続化
      - ./output/ai_ea:/workspace/ai_ea

    restart: unless-stopped

    # 共有メモリ (大バッチ学習時に必要)
    shm_size: "8gb"

    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
